{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_BreakOut_DQN_Conv.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10Pr-JPdviDa",
        "outputId": "e2488112-52b5-48fd-fa98-1295c304a2bb"
      },
      "source": [
        "# install required system dependencies\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install pyglet==v1.5.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyglet==v1.5.0 in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==v1.5.0) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3gDx9KDRh2m"
      },
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# Open AI related\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "# set up OpenAi Gym render in Colab\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "_display = Display(visible=False,  # use False with Xvfb\n",
        "                   size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GSJbtVZavN"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoGBBakfZe-6"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.999\n",
        "EPS_START = 1 # explore rate (start)\n",
        "EPS_END = 0.1 # explore rate (end)\n",
        "MAX_EPS_DECAY_STEPS = 1000000\n",
        "TARGET_UPDATE = 10 # Freqency of updating target network, copying all weights and biases in DQN\n",
        "REPLAY_MEMORY_SIZE = 1000000\n",
        "NUM_EPISODE = 12000\n",
        "k = 4 # Agenet select new action on every kth frame\n",
        "SCREEN_SIZE = 84 # height and width since we want training image to be a square"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAkSJXnZ6ZwX"
      },
      "source": [
        "## Show Gym Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX13PeR03duc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "a4f2bb0c-0772-4cfb-df3a-585ff10b797e"
      },
      "source": [
        "env = gym.make('Breakout-v0').unwrapped\n",
        "env.reset()\n",
        "img = plt.imshow(env.render(mode='rgb_array')) # returned screen by gym is 400x600x3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqN_MJb9d1mO",
        "outputId": "ee6f2226-61f7-4f61-a5a4-1781a4a2c016"
      },
      "source": [
        "print(\"number of actions: \", env.action_space.n)\n",
        "print(\"actions available: \", env.get_action_meanings())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of actions:  4\n",
            "actions available:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hlI85NlgNc8"
      },
      "source": [
        "class WrapAtariEnv(gym.Wrapper):\n",
        "    def __init__(self, env, frames_skipped=k, test = False):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.no_op_max = 30\n",
        "        self.frames_skipped = frames_skipped\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
        "                                                shape=(84, 84),\n",
        "                                                dtype=np.float32)\n",
        "        self.lives = 0\n",
        "        self.game_ended = True\n",
        "        self.test = test\n",
        "\n",
        "    def reset(self):\n",
        "        if not self.test:\n",
        "            # only reset on end of episode\n",
        "            if self.game_ended:\n",
        "                self.env.reset()\n",
        "                noops = np.random.randint(1, self.no_op_max + 1)\n",
        "                # execute the action no-op\n",
        "                for i in range(noops):\n",
        "                    ob, reward, done, info = self.env.step(0)\n",
        "                    if done:\n",
        "                        ob = self.reset()\n",
        "            else:\n",
        "                # execute no-op to advance from lost life state\n",
        "                ob, reward, done, info = self.env.step(0) \n",
        "        else:\n",
        "            self.env.reset()\n",
        "            noops = np.random.randint(1, self.no_op_max + 1)\n",
        "            # execute the action no-op\n",
        "            for i in range(noops):\n",
        "                ob, reward, done, info = self.env.step(0)  \n",
        "                if done:\n",
        "                    ob = self.reset()\n",
        "        \n",
        "        # process observation\n",
        "        return self.process_ob(ob)\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.\n",
        "        for i in range(self.frames_skipped):\n",
        "            ob, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "        if not self.test:\n",
        "            self.game_ended = done\n",
        "            lives_left = self.env.unwrapped.ale.lives()  # how many lives left\n",
        "            if lives_left < self.lives and lives_left > 0: # if agent lost a life\n",
        "                done = True\n",
        "            self.lives = lives_left\n",
        "        ob = self.process_ob(ob)\n",
        "        return ob, total_reward, done, info\n",
        "\n",
        "    def process_ob(self, ob):\n",
        "        # convert RGB image to Graysacle image and resize\n",
        "        im = cv2.cvtColor(ob, cv2.COLOR_RGB2GRAY)\n",
        "        ob = cv2.resize(im, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "        # Convert to float, rescale, convert to torch tensor\n",
        "        ob = np.ascontiguousarray(ob, dtype=np.float32) / 255\n",
        "        ob = torch.from_numpy(ob)\n",
        "        return ob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX0yQ5fq1CFh",
        "outputId": "fac8c15e-3c4c-4f3a-a2f3-a6363dd52cdb"
      },
      "source": [
        "env = WrapAtariEnv(env)\n",
        "env.reset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lF4pPBI_hgH"
      },
      "source": [
        "## Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0fr3w347npT"
      },
      "source": [
        "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x92xQonBntXT"
      },
      "source": [
        "## Deep Q Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3WY0aj078g4"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4) \n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Number of Linear input connections depends on output of conv2d layers\n",
        "        # and therefore the input image size, so compute it.\n",
        "        def conv2d_size_out(size, kernel_size, stride):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        \n",
        "        conv_size1 = conv2d_size_out(SCREEN_SIZE, 8, 4)\n",
        "        conv_size2 = conv2d_size_out(conv_size1, 4, 2)\n",
        "        conv_size3 = conv2d_size_out(conv_size2, 3, 1)\n",
        "\n",
        "        linear_input_size = conv_size3 * conv_size3 * 64\n",
        "        print(\"linear_input_size:\", linear_input_size)\n",
        "\n",
        "        # fully connected layor\n",
        "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, outputs)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # fully connected\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "supLL4d2Yqom"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9YdHcCdZR1Y",
        "outputId": "34117181-f39f-4005-9a25-4a6bf7dc4e4d"
      },
      "source": [
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(n_actions).to(device)\n",
        "target_net = DQN(n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
        "\n",
        "steps_done = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear_input_size: 3136\n",
            "linear_input_size: 3136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBUpKX0Eaz5F"
      },
      "source": [
        "def decay_epsilon(step):\n",
        "    # linearly decay epsilon\n",
        "    frac = min(float(step) / MAX_EPS_DECAY_STEPS, 1.0)\n",
        "    return EPS_START + frac*(EPS_END - EPS_START)\n",
        "\n",
        "def get_q_values(state):\n",
        "    return policy_net(state)\n",
        "\n",
        "def select_action(q_values):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = decay_epsilon(steps_done)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        # t.max(1) will return largest column value of each row.\n",
        "        # second column on max result is index of where max element was\n",
        "        # found, so we pick action with the larger expected reward.\n",
        "        return q_values.max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egpoL3jzbBbG"
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    next_state_values[non_final_mask] = next_state_values[non_final_mask].type(torch.float32)\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "    expected_state_action_values = expected_state_action_values.type(torch.float32)\n",
        "\n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    loss = loss.type(torch.float32)\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mym0Gh-T4dxJ"
      },
      "source": [
        "def stack_past_four_frames(frame1, frame2, frame3, frame4):\n",
        "    frames = torch.stack([frame1, frame2, frame3, frame4], dim=0)\n",
        "    # Add a batch dimension (BCHW)\n",
        "    return frames.unsqueeze(0).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSxdmob7bM6Z"
      },
      "source": [
        "episode_durations = []\n",
        "all_episode_max_q_vals = []\n",
        "all_episode_training_time = []\n",
        "all_episode_scores = []\n",
        "\n",
        "def train(num_episode, environment):\n",
        "    for i_episode in range(num_episode):\n",
        "        max_q_vals = []\n",
        "        total_score = 0\n",
        "\n",
        "        # Initialize the environment and state\n",
        "        environment.reset()\n",
        "        environment.step(1) # fire the ball\n",
        "        # Stack 4 most recent frames\n",
        "        past_ob3, _, _, _ = environment.step(0) # 0 -> no op\n",
        "        past_ob2, _, _, _ = environment.step(0)\n",
        "        past_ob1, _, _, _ = environment.step(0)\n",
        "        current_ob, _, _, _ = environment.step(0)\n",
        "        state = stack_past_four_frames(current_ob, past_ob1, past_ob2, past_ob3)\n",
        "\n",
        "        start = time.time()\n",
        "        for t in count():\n",
        "            print(\"episode {}, iteration {}\".format(i_episode, t))\n",
        "\n",
        "            environment.render()     \n",
        "            # Compute Q values from policy net\n",
        "            q_values = get_q_values(state)\n",
        "            print(\"q values: \", q_values)\n",
        "            max_q_vals.append(q_values.max().item())\n",
        "\n",
        "            # Choose action based on max q value\n",
        "            action = select_action(q_values)\n",
        "            print(\"Action: \", action.item())\n",
        "\n",
        "            # Execute action\n",
        "            new_ob, reward, done, info = environment.step(action.item())\n",
        "            total_score += reward\n",
        "            # convert reward to {+1, -1, 0}\n",
        "            reward = np.sign(reward)  \n",
        "            reward = torch.tensor([reward], device=device)\n",
        "            \n",
        "            print(\"reward: \", reward)\n",
        "            print(\"current total score: \", total_score)\n",
        "\n",
        "            # Observe new state\n",
        "            past_ob3 = past_ob2\n",
        "            past_ob2 = past_ob1\n",
        "            past_ob1 = current_ob\n",
        "            current_ob = new_ob\n",
        "            \n",
        "            if not done:\n",
        "                next_state = stack_past_four_frames(current_ob, past_ob1, past_ob2, past_ob3)\n",
        "            else:\n",
        "                next_state = None\n",
        "\n",
        "            # Store the transition in memory\n",
        "            memory.push(state, action, next_state, reward)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            optimize_model()\n",
        "            if done:\n",
        "                end = time.time()\n",
        "                all_episode_training_time.append(end - start)\n",
        "                episode_durations.append(t + 1)\n",
        "                all_episode_max_q_vals.append(max_q_vals)\n",
        "                all_episode_scores.append(total_score)\n",
        "                break\n",
        "        # Update the target network, copying all weights and biases in DQN\n",
        "        if i_episode % TARGET_UPDATE == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print('Training Complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSslOSxMyWTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42725cd-1f7a-483c-c0be-e31593c44304"
      },
      "source": [
        "train(NUM_EPISODE, env)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "episode 9780, iteration 3\n",
            "q values:  tensor([[0.4081, 0.3942, 0.4004, 0.3972]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9781, iteration 0\n",
            "q values:  tensor([[0.4010, 0.3829, 0.3901, 0.3975]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9781, iteration 1\n",
            "q values:  tensor([[0.4100, 0.3729, 0.3970, 0.3976]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9781, iteration 2\n",
            "q values:  tensor([[0.4239, 0.3772, 0.3871, 0.3978]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9782, iteration 0\n",
            "q values:  tensor([[0.4154, 0.3905, 0.3909, 0.3921]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9782, iteration 1\n",
            "q values:  tensor([[0.3952, 0.3857, 0.3867, 0.4071]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9783, iteration 0\n",
            "q values:  tensor([[0.3958, 0.4020, 0.3824, 0.3999]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9783, iteration 1\n",
            "q values:  tensor([[0.3996, 0.4111, 0.3970, 0.3940]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9783, iteration 2\n",
            "q values:  tensor([[0.3997, 0.4031, 0.3868, 0.3833]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9784, iteration 0\n",
            "q values:  tensor([[0.4147, 0.4181, 0.3962, 0.3878]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9784, iteration 1\n",
            "q values:  tensor([[0.4005, 0.4165, 0.3967, 0.3891]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9784, iteration 2\n",
            "q values:  tensor([[0.4152, 0.4081, 0.4128, 0.3845]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9784, iteration 3\n",
            "q values:  tensor([[0.4084, 0.4069, 0.4060, 0.4035]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9785, iteration 0\n",
            "q values:  tensor([[0.3896, 0.4062, 0.4004, 0.3972]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9785, iteration 1\n",
            "q values:  tensor([[0.3907, 0.3866, 0.3952, 0.4062]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9785, iteration 2\n",
            "q values:  tensor([[0.3923, 0.3877, 0.3802, 0.3997]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9785, iteration 3\n",
            "q values:  tensor([[0.3874, 0.3775, 0.3771, 0.3938]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9785, iteration 4\n",
            "q values:  tensor([[0.3892, 0.3789, 0.3931, 0.4034]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9785, iteration 5\n",
            "q values:  tensor([[0.3791, 0.3799, 0.3886, 0.4206]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9785, iteration 6\n",
            "q values:  tensor([[0.3754, 0.3726, 0.3788, 0.4191]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9785, iteration 7\n",
            "q values:  tensor([[0.3780, 0.3657, 0.3831, 0.4177]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 8\n",
            "q values:  tensor([[0.3890, 0.3860, 0.3740, 0.4154]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 9\n",
            "q values:  tensor([[0.3842, 0.4126, 0.3757, 0.4230]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 10\n",
            "q values:  tensor([[0.3746, 0.3934, 0.3901, 0.4200]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 11\n",
            "q values:  tensor([[0.3754, 0.3920, 0.3985, 0.4114]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 12\n",
            "q values:  tensor([[0.3785, 0.3926, 0.3961, 0.4188]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 13\n",
            "q values:  tensor([[0.3700, 0.4024, 0.3862, 0.4185]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 14\n",
            "q values:  tensor([[0.3685, 0.4050, 0.3870, 0.4171]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 15\n",
            "q values:  tensor([[0.3592, 0.3985, 0.3883, 0.4080]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 16\n",
            "q values:  tensor([[0.3520, 0.3988, 0.4025, 0.4009]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 17\n",
            "q values:  tensor([[0.3494, 0.3929, 0.4022, 0.4100]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9785, iteration 18\n",
            "q values:  tensor([[0.3723, 0.3872, 0.4153, 0.4324]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 0\n",
            "q values:  tensor([[0.3683, 0.3835, 0.4030, 0.4295]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9786, iteration 1\n",
            "q values:  tensor([[0.3647, 0.3800, 0.3974, 0.4226]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9786, iteration 2\n",
            "q values:  tensor([[0.3450, 0.3862, 0.3978, 0.4212]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9786, iteration 3\n",
            "q values:  tensor([[0.3490, 0.3873, 0.3875, 0.4197]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9786, iteration 4\n",
            "q values:  tensor([[0.3507, 0.3832, 0.3887, 0.4121]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9786, iteration 5\n",
            "q values:  tensor([[0.3598, 0.3791, 0.3974, 0.4042]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9786, iteration 6\n",
            "q values:  tensor([[0.3635, 0.3745, 0.4238, 0.4037]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9786, iteration 7\n",
            "q values:  tensor([[0.3754, 0.3707, 0.4178, 0.4215]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9786, iteration 8\n",
            "q values:  tensor([[0.3720, 0.3730, 0.4156, 0.4063]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 9\n",
            "q values:  tensor([[0.3842, 0.3859, 0.4162, 0.4058]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 10\n",
            "q values:  tensor([[0.3859, 0.3693, 0.4225, 0.4082]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 11\n",
            "q values:  tensor([[0.3882, 0.3758, 0.4093, 0.4076]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 12\n",
            "q values:  tensor([[0.3833, 0.3718, 0.4106, 0.4069]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 13\n",
            "q values:  tensor([[0.3849, 0.3968, 0.4232, 0.4064]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 14\n",
            "q values:  tensor([[0.3907, 0.3969, 0.4214, 0.3871]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 15\n",
            "q values:  tensor([[0.3857, 0.3910, 0.4266, 0.3828]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 16\n",
            "q values:  tensor([[0.3806, 0.3860, 0.4116, 0.3783]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 17\n",
            "q values:  tensor([[0.3715, 0.3757, 0.4111, 0.3691]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9786, iteration 18\n",
            "q values:  tensor([[0.3608, 0.3872, 0.4052, 0.3673]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9787, iteration 0\n",
            "q values:  tensor([[0.3570, 0.3883, 0.3939, 0.3673]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9787, iteration 1\n",
            "q values:  tensor([[0.3555, 0.3770, 0.3895, 0.3801]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9787, iteration 2\n",
            "q values:  tensor([[0.3563, 0.3726, 0.3906, 0.3983]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9787, iteration 3\n",
            "q values:  tensor([[0.3605, 0.3640, 0.3868, 0.3923]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9788, iteration 0\n",
            "q values:  tensor([[0.3669, 0.3680, 0.3890, 0.3927]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9788, iteration 1\n",
            "q values:  tensor([[0.3659, 0.3653, 0.3842, 0.3756]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9788, iteration 2\n",
            "q values:  tensor([[0.3586, 0.3698, 0.3755, 0.3772]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9788, iteration 3\n",
            "q values:  tensor([[0.3531, 0.3605, 0.3616, 0.3735]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9788, iteration 4\n",
            "q values:  tensor([[0.3593, 0.3744, 0.3583, 0.3723]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9788, iteration 5\n",
            "q values:  tensor([[0.3574, 0.3709, 0.3598, 0.3898]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9788, iteration 6\n",
            "q values:  tensor([[0.3514, 0.3756, 0.3766, 0.3845]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9788, iteration 7\n",
            "q values:  tensor([[0.3431, 0.3828, 0.3741, 0.3798]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 8\n",
            "q values:  tensor([[0.3497, 0.3726, 0.3614, 0.3817]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 9\n",
            "q values:  tensor([[0.3539, 0.3839, 0.3564, 0.3787]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 10\n",
            "q values:  tensor([[0.3873, 0.3796, 0.3545, 0.4007]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 11\n",
            "q values:  tensor([[0.3821, 0.4068, 0.3629, 0.4007]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 12\n",
            "q values:  tensor([[0.3713, 0.4153, 0.3752, 0.4099]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 13\n",
            "q values:  tensor([[0.3633, 0.4080, 0.3986, 0.4024]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 14\n",
            "q values:  tensor([[0.3605, 0.4006, 0.3881, 0.3960]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 15\n",
            "q values:  tensor([[0.3406, 0.4006, 0.3847, 0.3964]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 16\n",
            "q values:  tensor([[0.3502, 0.3912, 0.3806, 0.3906]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 17\n",
            "q values:  tensor([[0.3487, 0.3864, 0.3836, 0.3914]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9788, iteration 18\n",
            "q values:  tensor([[0.3489, 0.4174, 0.4068, 0.3797]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9789, iteration 0\n",
            "q values:  tensor([[0.3452, 0.4296, 0.4010, 0.3810]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9789, iteration 1\n",
            "q values:  tensor([[0.3350, 0.4269, 0.4009, 0.3768]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9789, iteration 2\n",
            "q values:  tensor([[0.3566, 0.4166, 0.3955, 0.4164]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9789, iteration 3\n",
            "q values:  tensor([[0.3510, 0.4087, 0.3905, 0.4085]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9789, iteration 4\n",
            "q values:  tensor([[0.3447, 0.4165, 0.3809, 0.4079]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9789, iteration 5\n",
            "q values:  tensor([[0.3517, 0.4090, 0.3728, 0.3897]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9789, iteration 6\n",
            "q values:  tensor([[0.3609, 0.4231, 0.3839, 0.3849]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9789, iteration 7\n",
            "q values:  tensor([[0.3757, 0.4075, 0.3802, 0.3807]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9789, iteration 8\n",
            "q values:  tensor([[0.3823, 0.4010, 0.3956, 0.3774]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9789, iteration 9\n",
            "q values:  tensor([[0.3716, 0.4128, 0.3905, 0.3828]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9789, iteration 10\n",
            "q values:  tensor([[0.3685, 0.4057, 0.3815, 0.3843]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9789, iteration 11\n",
            "q values:  tensor([[0.3610, 0.3930, 0.3830, 0.3798]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9789, iteration 12\n",
            "q values:  tensor([[0.3596, 0.3707, 0.3854, 0.3811]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9790, iteration 0\n",
            "q values:  tensor([[0.3629, 0.3679, 0.3814, 0.3993]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9790, iteration 1\n",
            "q values:  tensor([[0.3967, 0.3664, 0.3619, 0.3994]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9790, iteration 2\n",
            "q values:  tensor([[0.4063, 0.3581, 0.3612, 0.3995]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9791, iteration 0\n",
            "q values:  tensor([[0.3939, 0.3500, 0.3649, 0.4230]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9791, iteration 1\n",
            "q values:  tensor([[0.3969, 0.3461, 0.3620, 0.4047]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9791, iteration 2\n",
            "q values:  tensor([[0.3933, 0.3489, 0.3670, 0.3780]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9792, iteration 0\n",
            "q values:  tensor([[0.3904, 0.3544, 0.3688, 0.3909]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9792, iteration 1\n",
            "q values:  tensor([[0.3934, 0.3509, 0.3841, 0.3877]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9792, iteration 2\n",
            "q values:  tensor([[0.4043, 0.3532, 0.3899, 0.3987]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9793, iteration 0\n",
            "q values:  tensor([[0.4004, 0.3762, 0.4001, 0.4013]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9793, iteration 1\n",
            "q values:  tensor([[0.4125, 0.3796, 0.4225, 0.4062]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9793, iteration 2\n",
            "q values:  tensor([[0.3951, 0.3778, 0.4359, 0.4211]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9794, iteration 0\n",
            "q values:  tensor([[0.3861, 0.3692, 0.4339, 0.4292]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9794, iteration 1\n",
            "q values:  tensor([[0.4032, 0.3629, 0.4262, 0.4287]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9794, iteration 2\n",
            "q values:  tensor([[0.4052, 0.3664, 0.4202, 0.4281]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9794, iteration 3\n",
            "q values:  tensor([[0.4007, 0.3862, 0.4226, 0.4177]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9795, iteration 0\n",
            "q values:  tensor([[0.4126, 0.3889, 0.4304, 0.4265]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9795, iteration 1\n",
            "q values:  tensor([[0.4135, 0.3741, 0.4238, 0.4199]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9795, iteration 2\n",
            "q values:  tensor([[0.4107, 0.3792, 0.4237, 0.4143]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9795, iteration 3\n",
            "q values:  tensor([[0.4057, 0.3774, 0.4120, 0.4151]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9796, iteration 0\n",
            "q values:  tensor([[0.3954, 0.3695, 0.4131, 0.4098]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9796, iteration 1\n",
            "q values:  tensor([[0.3859, 0.3703, 0.4219, 0.4052]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9796, iteration 2\n",
            "q values:  tensor([[0.3867, 0.3647, 0.4219, 0.4006]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9797, iteration 0\n",
            "q values:  tensor([[0.3846, 0.3615, 0.4358, 0.4022]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9797, iteration 1\n",
            "q values:  tensor([[0.3857, 0.3625, 0.4284, 0.4031]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9797, iteration 2\n",
            "q values:  tensor([[0.4070, 0.3588, 0.4416, 0.4039]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9797, iteration 3\n",
            "q values:  tensor([[0.4025, 0.3646, 0.4489, 0.4140]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9798, iteration 0\n",
            "q values:  tensor([[0.3911, 0.3744, 0.4459, 0.4088]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9798, iteration 1\n",
            "q values:  tensor([[0.3973, 0.3884, 0.4704, 0.3978]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9798, iteration 2\n",
            "q values:  tensor([[0.4004, 0.3807, 0.4672, 0.4229]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9799, iteration 0\n",
            "q values:  tensor([[0.4025, 0.3946, 0.4578, 0.4250]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9799, iteration 1\n",
            "q values:  tensor([[0.3922, 0.3864, 0.4540, 0.4128]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9799, iteration 2\n",
            "q values:  tensor([[0.3960, 0.3705, 0.4505, 0.4142]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9799, iteration 3\n",
            "q values:  tensor([[0.4133, 0.3647, 0.4416, 0.4091]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9800, iteration 0\n",
            "q values:  tensor([[0.3955, 0.3787, 0.4406, 0.4113]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9800, iteration 1\n",
            "q values:  tensor([[0.3932, 0.3920, 0.4270, 0.4000]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9800, iteration 2\n",
            "q values:  tensor([[0.3723, 0.3953, 0.4150, 0.3843]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9800, iteration 3\n",
            "q values:  tensor([[0.3698, 0.4123, 0.4048, 0.3826]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9801, iteration 0\n",
            "q values:  tensor([[0.3972, 0.4078, 0.3999, 0.3873]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9801, iteration 1\n",
            "q values:  tensor([[0.3862, 0.4016, 0.3900, 0.3982]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9801, iteration 2\n",
            "q values:  tensor([[0.3880, 0.4021, 0.4007, 0.3874]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9802, iteration 0\n",
            "q values:  tensor([[0.3908, 0.4023, 0.4145, 0.3891]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9802, iteration 1\n",
            "q values:  tensor([[0.3809, 0.4031, 0.4082, 0.3791]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9802, iteration 2\n",
            "q values:  tensor([[0.3867, 0.4001, 0.4082, 0.3773]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9802, iteration 3\n",
            "q values:  tensor([[0.4031, 0.4009, 0.4162, 0.3948]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9802, iteration 4\n",
            "q values:  tensor([[0.3918, 0.4105, 0.4037, 0.3959]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9802, iteration 5\n",
            "q values:  tensor([[0.3868, 0.4038, 0.4068, 0.3913]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9802, iteration 6\n",
            "q values:  tensor([[0.3883, 0.3916, 0.4069, 0.3820]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9802, iteration 7\n",
            "q values:  tensor([[0.3906, 0.3814, 0.3956, 0.3782]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9802, iteration 8\n",
            "q values:  tensor([[0.3923, 0.3804, 0.3977, 0.3804]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9802, iteration 9\n",
            "q values:  tensor([[0.3814, 0.3933, 0.3986, 0.3787]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9802, iteration 10\n",
            "q values:  tensor([[0.3955, 0.3881, 0.3938, 0.3709]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9802, iteration 11\n",
            "q values:  tensor([[0.3971, 0.3896, 0.3955, 0.3758]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9803, iteration 0\n",
            "q values:  tensor([[0.3865, 0.3799, 0.3964, 0.3870]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9803, iteration 1\n",
            "q values:  tensor([[0.3776, 0.3761, 0.4108, 0.3994]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9803, iteration 2\n",
            "q values:  tensor([[0.4030, 0.3810, 0.4247, 0.3885]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9803, iteration 3\n",
            "q values:  tensor([[0.4178, 0.3991, 0.4163, 0.4044]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9804, iteration 0\n",
            "q values:  tensor([[0.4050, 0.4246, 0.4096, 0.4045]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9804, iteration 1\n",
            "q values:  tensor([[0.4053, 0.4225, 0.3979, 0.4049]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9804, iteration 2\n",
            "q values:  tensor([[0.4223, 0.4079, 0.3820, 0.4051]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9805, iteration 0\n",
            "q values:  tensor([[0.4286, 0.4016, 0.3780, 0.3992]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9805, iteration 1\n",
            "q values:  tensor([[0.4213, 0.4116, 0.3748, 0.4090]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9805, iteration 2\n",
            "q values:  tensor([[0.4195, 0.4110, 0.3872, 0.4239]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9805, iteration 3\n",
            "q values:  tensor([[0.4124, 0.4043, 0.3895, 0.4221]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9806, iteration 0\n",
            "q values:  tensor([[0.4062, 0.3921, 0.4004, 0.4136]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9806, iteration 1\n",
            "q values:  tensor([[0.4415, 0.3937, 0.3896, 0.4279]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9806, iteration 2\n",
            "q values:  tensor([[0.4107, 0.3829, 0.3856, 0.4406]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9807, iteration 0\n",
            "q values:  tensor([[0.3998, 0.3992, 0.3910, 0.4237]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9807, iteration 1\n",
            "q values:  tensor([[0.4004, 0.4002, 0.3806, 0.4086]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9807, iteration 2\n",
            "q values:  tensor([[0.3903, 0.4039, 0.3836, 0.3851]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9807, iteration 3\n",
            "q values:  tensor([[0.3769, 0.3982, 0.3747, 0.3802]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9807, iteration 4\n",
            "q values:  tensor([[0.3639, 0.3998, 0.4044, 0.3771]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9807, iteration 5\n",
            "q values:  tensor([[0.3744, 0.4306, 0.4183, 0.3883]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9807, iteration 6\n",
            "q values:  tensor([[0.3729, 0.4285, 0.4058, 0.3987]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9807, iteration 7\n",
            "q values:  tensor([[0.3726, 0.4149, 0.4030, 0.3937]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 8\n",
            "q values:  tensor([[0.3646, 0.4023, 0.4248, 0.3729]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 9\n",
            "q values:  tensor([[0.3643, 0.3970, 0.4258, 0.3710]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 10\n",
            "q values:  tensor([[0.3914, 0.4016, 0.4500, 0.3736]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 11\n",
            "q values:  tensor([[0.3781, 0.4023, 0.4477, 0.3662]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 12\n",
            "q values:  tensor([[0.3898, 0.4027, 0.4434, 0.3494]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 13\n",
            "q values:  tensor([[0.3993, 0.3968, 0.4529, 0.3585]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 14\n",
            "q values:  tensor([[0.3904, 0.3861, 0.4492, 0.3541]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 15\n",
            "q values:  tensor([[0.3820, 0.3897, 0.4470, 0.3427]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 16\n",
            "q values:  tensor([[0.3840, 0.3864, 0.4427, 0.3375]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 17\n",
            "q values:  tensor([[0.3809, 0.3769, 0.4384, 0.3536]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 18\n",
            "q values:  tensor([[0.3836, 0.3749, 0.4359, 0.3605]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 19\n",
            "q values:  tensor([[0.3716, 0.4018, 0.4288, 0.3652]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 20\n",
            "q values:  tensor([[0.3700, 0.4024, 0.4149, 0.3577]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 21\n",
            "q values:  tensor([[0.3672, 0.4029, 0.4515, 0.3554]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 22\n",
            "q values:  tensor([[0.3708, 0.4125, 0.4476, 0.3599]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9807, iteration 23\n",
            "q values:  tensor([[0.3770, 0.4056, 0.4383, 0.3706]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9807, iteration 24\n",
            "q values:  tensor([[0.3802, 0.3936, 0.4359, 0.3695]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9807, iteration 25\n",
            "q values:  tensor([[0.3831, 0.3955, 0.4337, 0.3825]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9807, iteration 26\n",
            "q values:  tensor([[0.3799, 0.3906, 0.4146, 0.3850]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9807, iteration 27\n",
            "q values:  tensor([[0.3857, 0.3924, 0.4083, 0.3860]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9807, iteration 28\n",
            "q values:  tensor([[0.3886, 0.3756, 0.3978, 0.3819]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9807, iteration 29\n",
            "q values:  tensor([[0.3795, 0.3748, 0.3934, 0.3932]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9807, iteration 30\n",
            "q values:  tensor([[0.3669, 0.3715, 0.4301, 0.3947]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9807, iteration 31\n",
            "q values:  tensor([[0.3545, 0.3776, 0.4180, 0.3955]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9807, iteration 32\n",
            "q values:  tensor([[0.3665, 0.3799, 0.4124, 0.4065]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9807, iteration 33\n",
            "q values:  tensor([[0.3692, 0.3774, 0.4141, 0.4006]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9808, iteration 0\n",
            "q values:  tensor([[0.3576, 0.3695, 0.4036, 0.4011]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9808, iteration 1\n",
            "q values:  tensor([[0.3609, 0.3679, 0.3917, 0.4019]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9808, iteration 2\n",
            "q values:  tensor([[0.3598, 0.3716, 0.3958, 0.4113]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9808, iteration 3\n",
            "q values:  tensor([[0.3636, 0.3691, 0.3824, 0.4013]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9809, iteration 0\n",
            "q values:  tensor([[0.3689, 0.3675, 0.3796, 0.4020]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9809, iteration 1\n",
            "q values:  tensor([[0.3795, 0.3547, 0.3728, 0.4115]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9809, iteration 2\n",
            "q values:  tensor([[0.3770, 0.3605, 0.3766, 0.4138]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9809, iteration 3\n",
            "q values:  tensor([[0.3740, 0.3425, 0.3659, 0.4071]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9809, iteration 4\n",
            "q values:  tensor([[0.3674, 0.3379, 0.3640, 0.3947]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9809, iteration 5\n",
            "q values:  tensor([[0.3809, 0.3317, 0.3787, 0.3841]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9809, iteration 6\n",
            "q values:  tensor([[0.3679, 0.3296, 0.4045, 0.3855]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9809, iteration 7\n",
            "q values:  tensor([[0.3627, 0.3369, 0.4164, 0.3879]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 8\n",
            "q values:  tensor([[0.3688, 0.3193, 0.4062, 0.3840]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 9\n",
            "q values:  tensor([[0.3715, 0.3131, 0.4063, 0.3862]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 10\n",
            "q values:  tensor([[0.3885, 0.3383, 0.3921, 0.4037]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 11\n",
            "q values:  tensor([[0.3855, 0.3593, 0.3933, 0.3917]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 12\n",
            "q values:  tensor([[0.3814, 0.3522, 0.3871, 0.4119]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 13\n",
            "q values:  tensor([[0.3920, 0.3519, 0.3960, 0.3932]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 14\n",
            "q values:  tensor([[0.3823, 0.3541, 0.3874, 0.4092]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 15\n",
            "q values:  tensor([[0.3739, 0.3629, 0.3884, 0.4236]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 16\n",
            "q values:  tensor([[0.3757, 0.3690, 0.3900, 0.4089]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 17\n",
            "q values:  tensor([[0.3740, 0.3686, 0.3863, 0.4028]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9809, iteration 18\n",
            "q values:  tensor([[0.3841, 0.3883, 0.3834, 0.4035]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9810, iteration 0\n",
            "q values:  tensor([[0.3692, 0.3844, 0.3817, 0.3919]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9810, iteration 1\n",
            "q values:  tensor([[0.3566, 0.3797, 0.3911, 0.3768]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9810, iteration 2\n",
            "q values:  tensor([[0.3756, 0.3773, 0.3996, 0.3795]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9810, iteration 3\n",
            "q values:  tensor([[0.3668, 0.3729, 0.4128, 0.3998]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9811, iteration 0\n",
            "q values:  tensor([[0.3684, 0.3698, 0.4124, 0.3785]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9811, iteration 1\n",
            "q values:  tensor([[0.3726, 0.3744, 0.4023, 0.3711]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9811, iteration 2\n",
            "q values:  tensor([[0.3707, 0.3663, 0.4031, 0.3769]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9811, iteration 3\n",
            "q values:  tensor([[0.3771, 0.3712, 0.4012, 0.3745]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9812, iteration 0\n",
            "q values:  tensor([[0.3738, 0.3757, 0.4096, 0.3877]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9812, iteration 1\n",
            "q values:  tensor([[0.3765, 0.3813, 0.4171, 0.3871]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9812, iteration 2\n",
            "q values:  tensor([[0.3732, 0.4093, 0.4114, 0.3778]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9812, iteration 3\n",
            "q values:  tensor([[0.3765, 0.4331, 0.4115, 0.3694]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9812, iteration 4\n",
            "q values:  tensor([[0.4063, 0.4236, 0.4115, 0.3743]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9812, iteration 5\n",
            "q values:  tensor([[0.3904, 0.4164, 0.4137, 0.3870]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9812, iteration 6\n",
            "q values:  tensor([[0.3927, 0.4098, 0.4033, 0.3906]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9812, iteration 7\n",
            "q values:  tensor([[0.3935, 0.3980, 0.3892, 0.3928]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9812, iteration 8\n",
            "q values:  tensor([[0.3945, 0.3996, 0.3991, 0.4038]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9812, iteration 9\n",
            "q values:  tensor([[0.3898, 0.3953, 0.3904, 0.4190]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9812, iteration 10\n",
            "q values:  tensor([[0.3922, 0.3909, 0.3863, 0.4117]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9812, iteration 11\n",
            "q values:  tensor([[0.3839, 0.4006, 0.3840, 0.4058]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9812, iteration 12\n",
            "q values:  tensor([[0.3810, 0.4104, 0.3757, 0.4416]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9812, iteration 13\n",
            "q values:  tensor([[0.3831, 0.4107, 0.3799, 0.4328]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9812, iteration 14\n",
            "q values:  tensor([[0.4000, 0.4046, 0.3735, 0.4255]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9812, iteration 15\n",
            "q values:  tensor([[0.3956, 0.3934, 0.3764, 0.4186]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9812, iteration 16\n",
            "q values:  tensor([[0.3915, 0.3883, 0.3884, 0.4010]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9812, iteration 17\n",
            "q values:  tensor([[0.3932, 0.3919, 0.4026, 0.3992]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9812, iteration 18\n",
            "q values:  tensor([[0.4088, 0.3884, 0.3882, 0.3889]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9812, iteration 19\n",
            "q values:  tensor([[0.4034, 0.3743, 0.3802, 0.3898]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9812, iteration 20\n",
            "q values:  tensor([[0.4179, 0.3704, 0.3778, 0.3851]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9812, iteration 21\n",
            "q values:  tensor([[0.4117, 0.3897, 0.3708, 0.3719]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9812, iteration 22\n",
            "q values:  tensor([[0.4255, 0.3804, 0.3692, 0.3718]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9812, iteration 23\n",
            "q values:  tensor([[0.4238, 0.3708, 0.3678, 0.3678]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9813, iteration 0\n",
            "q values:  tensor([[0.4108, 0.3739, 0.3623, 0.3613]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9813, iteration 1\n",
            "q values:  tensor([[0.4053, 0.3655, 0.3567, 0.3612]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9813, iteration 2\n",
            "q values:  tensor([[0.3892, 0.3697, 0.3672, 0.3942]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9813, iteration 3\n",
            "q values:  tensor([[0.4000, 0.3736, 0.3559, 0.3927]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9814, iteration 0\n",
            "q values:  tensor([[0.3959, 0.3675, 0.3556, 0.4069]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9814, iteration 1\n",
            "q values:  tensor([[0.4048, 0.3736, 0.3644, 0.3965]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9814, iteration 2\n",
            "q values:  tensor([[0.3997, 0.3726, 0.3788, 0.3920]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9814, iteration 3\n",
            "q values:  tensor([[0.4011, 0.3699, 0.3728, 0.4014]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9815, iteration 0\n",
            "q values:  tensor([[0.3906, 0.3700, 0.3710, 0.4156]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9815, iteration 1\n",
            "q values:  tensor([[0.4002, 0.3765, 0.3710, 0.4099]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9815, iteration 2\n",
            "q values:  tensor([[0.3956, 0.3912, 0.3677, 0.4047]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9816, iteration 0\n",
            "q values:  tensor([[0.3817, 0.4223, 0.3675, 0.4049]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9816, iteration 1\n",
            "q values:  tensor([[0.3780, 0.4360, 0.3540, 0.4078]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9816, iteration 2\n",
            "q values:  tensor([[0.3699, 0.4211, 0.3539, 0.4106]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9816, iteration 3\n",
            "q values:  tensor([[0.3637, 0.4140, 0.3508, 0.4108]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9817, iteration 0\n",
            "q values:  tensor([[0.3602, 0.4018, 0.3526, 0.4133]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9817, iteration 1\n",
            "q values:  tensor([[0.3573, 0.4031, 0.3574, 0.4075]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9817, iteration 2\n",
            "q values:  tensor([[0.3461, 0.3810, 0.3689, 0.4078]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9817, iteration 3\n",
            "q values:  tensor([[0.3608, 0.3773, 0.3686, 0.3971]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9817, iteration 4\n",
            "q values:  tensor([[0.3775, 0.3698, 0.3622, 0.3879]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9817, iteration 5\n",
            "q values:  tensor([[0.3746, 0.3663, 0.3664, 0.3689]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9817, iteration 6\n",
            "q values:  tensor([[0.4053, 0.3629, 0.3683, 0.3851]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9817, iteration 7\n",
            "q values:  tensor([[0.4002, 0.3506, 0.3772, 0.3821]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9817, iteration 8\n",
            "q values:  tensor([[0.4090, 0.3533, 0.3773, 0.3696]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9817, iteration 9\n",
            "q values:  tensor([[0.3984, 0.3593, 0.3797, 0.3942]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9817, iteration 10\n",
            "q values:  tensor([[0.3899, 0.3703, 0.3810, 0.3913]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9817, iteration 11\n",
            "q values:  tensor([[0.4008, 0.3820, 0.3730, 0.3878]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9817, iteration 12\n",
            "q values:  tensor([[0.4092, 0.3730, 0.3803, 0.3856]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9817, iteration 13\n",
            "q values:  tensor([[0.4095, 0.3764, 0.3786, 0.4018]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9817, iteration 14\n",
            "q values:  tensor([[0.3986, 0.3745, 0.3751, 0.3980]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9817, iteration 15\n",
            "q values:  tensor([[0.4207, 0.3734, 0.3714, 0.4000]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9817, iteration 16\n",
            "q values:  tensor([[0.4163, 0.3894, 0.3893, 0.3853]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9817, iteration 17\n",
            "q values:  tensor([[0.4181, 0.3920, 0.4050, 0.3827]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9817, iteration 18\n",
            "q values:  tensor([[0.4065, 0.3938, 0.4196, 0.3985]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9817, iteration 19\n",
            "q values:  tensor([[0.4069, 0.4048, 0.3961, 0.3994]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9818, iteration 0\n",
            "q values:  tensor([[0.4019, 0.4201, 0.3872, 0.3951]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9818, iteration 1\n",
            "q values:  tensor([[0.3968, 0.3947, 0.3843, 0.3967]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9818, iteration 2\n",
            "q values:  tensor([[0.3929, 0.3965, 0.3784, 0.3918]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9819, iteration 0\n",
            "q values:  tensor([[0.3838, 0.3805, 0.3829, 0.3934]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9819, iteration 1\n",
            "q values:  tensor([[0.4002, 0.3930, 0.3937, 0.3947]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9819, iteration 2\n",
            "q values:  tensor([[0.3958, 0.3887, 0.3903, 0.3914]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9820, iteration 0\n",
            "q values:  tensor([[0.4107, 0.4008, 0.3811, 0.3872]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9820, iteration 1\n",
            "q values:  tensor([[0.4107, 0.4104, 0.3651, 0.3839]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9820, iteration 2\n",
            "q values:  tensor([[0.4108, 0.4131, 0.3596, 0.3870]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9821, iteration 0\n",
            "q values:  tensor([[0.4054, 0.4130, 0.3492, 0.3905]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9821, iteration 1\n",
            "q values:  tensor([[0.4006, 0.4130, 0.3431, 0.3830]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9822, iteration 0\n",
            "q values:  tensor([[0.4043, 0.4069, 0.3330, 0.3798]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9822, iteration 1\n",
            "q values:  tensor([[0.4315, 0.4107, 0.3271, 0.3816]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9822, iteration 2\n",
            "q values:  tensor([[0.4247, 0.4195, 0.3219, 0.3871]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9823, iteration 0\n",
            "q values:  tensor([[0.4376, 0.4187, 0.3299, 0.3852]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9823, iteration 1\n",
            "q values:  tensor([[0.4350, 0.4183, 0.3322, 0.3899]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9823, iteration 2\n",
            "q values:  tensor([[0.4343, 0.4261, 0.3389, 0.3856]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9824, iteration 0\n",
            "q values:  tensor([[0.4271, 0.4251, 0.3471, 0.3674]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9824, iteration 1\n",
            "q values:  tensor([[0.4387, 0.4111, 0.3653, 0.3856]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9824, iteration 2\n",
            "q values:  tensor([[0.4249, 0.4052, 0.3629, 0.3833]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9824, iteration 3\n",
            "q values:  tensor([[0.4123, 0.4147, 0.3737, 0.3698]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9825, iteration 0\n",
            "q values:  tensor([[0.4014, 0.4292, 0.3728, 0.3722]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9825, iteration 1\n",
            "q values:  tensor([[0.3863, 0.4423, 0.3710, 0.3721]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9825, iteration 2\n",
            "q values:  tensor([[0.4015, 0.4392, 0.3658, 0.3760]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9825, iteration 3\n",
            "q values:  tensor([[0.3920, 0.4364, 0.3773, 0.3831]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9825, iteration 4\n",
            "q values:  tensor([[0.3933, 0.4346, 0.3760, 0.3827]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9825, iteration 5\n",
            "q values:  tensor([[0.3943, 0.4327, 0.3648, 0.4005]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9825, iteration 6\n",
            "q values:  tensor([[0.3807, 0.4252, 0.3763, 0.4161]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9825, iteration 7\n",
            "q values:  tensor([[0.3786, 0.4259, 0.3881, 0.4296]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9825, iteration 8\n",
            "q values:  tensor([[0.3713, 0.4113, 0.3684, 0.4287]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9825, iteration 9\n",
            "q values:  tensor([[0.3856, 0.3990, 0.3925, 0.4223]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9825, iteration 10\n",
            "q values:  tensor([[0.3944, 0.4003, 0.3841, 0.4346]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9825, iteration 11\n",
            "q values:  tensor([[0.4038, 0.3895, 0.3820, 0.4213]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9825, iteration 12\n",
            "q values:  tensor([[0.3832, 0.4081, 0.3994, 0.4088]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9825, iteration 13\n",
            "q values:  tensor([[0.3847, 0.4086, 0.3964, 0.4115]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9826, iteration 0\n",
            "q values:  tensor([[0.4231, 0.4181, 0.3923, 0.4058]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9826, iteration 1\n",
            "q values:  tensor([[0.4341, 0.3993, 0.3785, 0.4008]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9826, iteration 2\n",
            "q values:  tensor([[0.4386, 0.3950, 0.3817, 0.3960]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9826, iteration 3\n",
            "q values:  tensor([[0.4200, 0.3914, 0.3963, 0.3916]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9827, iteration 0\n",
            "q values:  tensor([[0.4197, 0.4087, 0.3870, 0.3721]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9827, iteration 1\n",
            "q values:  tensor([[0.4158, 0.3971, 0.3884, 0.3703]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9827, iteration 2\n",
            "q values:  tensor([[0.4074, 0.3872, 0.3900, 0.3669]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9827, iteration 3\n",
            "q values:  tensor([[0.4028, 0.3844, 0.4004, 0.3721]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9827, iteration 4\n",
            "q values:  tensor([[0.4034, 0.3899, 0.3960, 0.3702]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9827, iteration 5\n",
            "q values:  tensor([[0.3990, 0.3861, 0.3924, 0.3704]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9827, iteration 6\n",
            "q values:  tensor([[0.3951, 0.3823, 0.4233, 0.3740]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9827, iteration 7\n",
            "q values:  tensor([[0.3921, 0.3731, 0.4168, 0.3720]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9827, iteration 8\n",
            "q values:  tensor([[0.3893, 0.3636, 0.4056, 0.3601]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9827, iteration 9\n",
            "q values:  tensor([[0.3907, 0.3593, 0.3851, 0.3669]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9827, iteration 10\n",
            "q values:  tensor([[0.3935, 0.3579, 0.3872, 0.3616]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9827, iteration 11\n",
            "q values:  tensor([[0.3905, 0.3558, 0.4029, 0.3615]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9827, iteration 12\n",
            "q values:  tensor([[0.3905, 0.3464, 0.4165, 0.3622]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9827, iteration 13\n",
            "q values:  tensor([[0.3922, 0.3535, 0.4003, 0.3614]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9827, iteration 14\n",
            "q values:  tensor([[0.3832, 0.3687, 0.3969, 0.3538]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9827, iteration 15\n",
            "q values:  tensor([[0.3851, 0.3560, 0.3937, 0.3683]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9827, iteration 16\n",
            "q values:  tensor([[0.3826, 0.3583, 0.3855, 0.3589]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9827, iteration 17\n",
            "q values:  tensor([[0.3846, 0.3904, 0.3874, 0.3772]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9827, iteration 18\n",
            "q values:  tensor([[0.3807, 0.3860, 0.3863, 0.3902]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9828, iteration 0\n",
            "q values:  tensor([[0.3859, 0.3800, 0.3783, 0.4137]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9828, iteration 1\n",
            "q values:  tensor([[0.3796, 0.3824, 0.3752, 0.3964]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9828, iteration 2\n",
            "q values:  tensor([[0.3764, 0.3812, 0.3919, 0.3980]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9829, iteration 0\n",
            "q values:  tensor([[0.3773, 0.4015, 0.4021, 0.3942]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9829, iteration 1\n",
            "q values:  tensor([[0.3801, 0.3971, 0.4106, 0.3905]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9829, iteration 2\n",
            "q values:  tensor([[0.3959, 0.4127, 0.3901, 0.3924]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9829, iteration 3\n",
            "q values:  tensor([[0.3924, 0.4067, 0.3776, 0.3885]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9830, iteration 0\n",
            "q values:  tensor([[0.3948, 0.4014, 0.3754, 0.3875]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9830, iteration 1\n",
            "q values:  tensor([[0.3963, 0.3845, 0.3730, 0.3807]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9830, iteration 2\n",
            "q values:  tensor([[0.3885, 0.3865, 0.3750, 0.3785]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9830, iteration 3\n",
            "q values:  tensor([[0.4050, 0.3839, 0.3621, 0.3695]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9831, iteration 0\n",
            "q values:  tensor([[0.4004, 0.3817, 0.3654, 0.3886]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9831, iteration 1\n",
            "q values:  tensor([[0.3950, 0.3732, 0.3633, 0.4036]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9831, iteration 2\n",
            "q values:  tensor([[0.4036, 0.3654, 0.3572, 0.4034]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9832, iteration 0\n",
            "q values:  tensor([[0.3925, 0.3597, 0.3611, 0.3974]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9832, iteration 1\n",
            "q values:  tensor([[0.3881, 0.3603, 0.3590, 0.3862]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9832, iteration 2\n",
            "q values:  tensor([[0.4027, 0.3532, 0.3476, 0.3876]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9832, iteration 3\n",
            "q values:  tensor([[0.3972, 0.3549, 0.3503, 0.3897]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9832, iteration 4\n",
            "q values:  tensor([[0.3974, 0.3589, 0.3418, 0.3855]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9832, iteration 5\n",
            "q values:  tensor([[0.4188, 0.3531, 0.3381, 0.4006]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9832, iteration 6\n",
            "q values:  tensor([[0.4180, 0.3442, 0.3455, 0.3948]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9832, iteration 7\n",
            "q values:  tensor([[0.4168, 0.3551, 0.3570, 0.3896]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9832, iteration 8\n",
            "q values:  tensor([[0.4092, 0.3668, 0.3557, 0.3853]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9832, iteration 9\n",
            "q values:  tensor([[0.3980, 0.3723, 0.3636, 0.3868]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9832, iteration 10\n",
            "q values:  tensor([[0.3875, 0.3761, 0.3643, 0.3821]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9832, iteration 11\n",
            "q values:  tensor([[0.3922, 0.3782, 0.3795, 0.3842]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9832, iteration 12\n",
            "q values:  tensor([[0.3826, 0.3677, 0.3806, 0.3956]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9832, iteration 13\n",
            "q values:  tensor([[0.3840, 0.3788, 0.3863, 0.3731]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9832, iteration 14\n",
            "q values:  tensor([[0.3941, 0.3957, 0.3956, 0.3701]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9832, iteration 15\n",
            "q values:  tensor([[0.3838, 0.3901, 0.3935, 0.3735]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9832, iteration 16\n",
            "q values:  tensor([[0.3794, 0.3735, 0.3972, 0.3758]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9832, iteration 17\n",
            "q values:  tensor([[0.3703, 0.3767, 0.4056, 0.3787]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9832, iteration 18\n",
            "q values:  tensor([[0.3899, 0.3735, 0.3997, 0.3747]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9832, iteration 19\n",
            "q values:  tensor([[0.3754, 0.3763, 0.4079, 0.3769]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9833, iteration 0\n",
            "q values:  tensor([[0.3717, 0.3731, 0.4015, 0.3949]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9833, iteration 1\n",
            "q values:  tensor([[0.3831, 0.3642, 0.4150, 0.4098]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9833, iteration 2\n",
            "q values:  tensor([[0.3756, 0.3636, 0.4273, 0.4095]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9833, iteration 3\n",
            "q values:  tensor([[0.3855, 0.3624, 0.4317, 0.4174]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9834, iteration 0\n",
            "q values:  tensor([[0.3947, 0.3707, 0.4192, 0.4146]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9834, iteration 1\n",
            "q values:  tensor([[0.3955, 0.3673, 0.4309, 0.4070]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9834, iteration 2\n",
            "q values:  tensor([[0.3852, 0.3671, 0.4225, 0.4210]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9835, iteration 0\n",
            "q values:  tensor([[0.3814, 0.3635, 0.4137, 0.4274]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9835, iteration 1\n",
            "q values:  tensor([[0.3795, 0.3544, 0.4072, 0.4191]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9835, iteration 2\n",
            "q values:  tensor([[0.3820, 0.3707, 0.4009, 0.4182]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9835, iteration 3\n",
            "q values:  tensor([[0.3787, 0.3748, 0.3845, 0.4049]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9835, iteration 4\n",
            "q values:  tensor([[0.3759, 0.3933, 0.3758, 0.3987]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9835, iteration 5\n",
            "q values:  tensor([[0.3672, 0.4033, 0.3723, 0.4272]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 6\n",
            "q values:  tensor([[0.3951, 0.4057, 0.3831, 0.4245]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 7\n",
            "q values:  tensor([[0.3901, 0.3929, 0.3984, 0.4293]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 8\n",
            "q values:  tensor([[0.4069, 0.3878, 0.4067, 0.4210]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 9\n",
            "q values:  tensor([[0.4066, 0.3870, 0.4010, 0.4199]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 10\n",
            "q values:  tensor([[0.4085, 0.3896, 0.3954, 0.4191]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 11\n",
            "q values:  tensor([[0.4157, 0.4062, 0.4175, 0.4182]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 12\n",
            "q values:  tensor([[0.4036, 0.4057, 0.4234, 0.4153]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 13\n",
            "q values:  tensor([[0.4032, 0.3989, 0.4412, 0.4079]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 14\n",
            "q values:  tensor([[0.3975, 0.3928, 0.4274, 0.3977]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 15\n",
            "q values:  tensor([[0.3924, 0.3939, 0.4144, 0.3862]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 16\n",
            "q values:  tensor([[0.3775, 0.3882, 0.4072, 0.3753]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 17\n",
            "q values:  tensor([[0.3551, 0.3832, 0.4013, 0.3771]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9835, iteration 18\n",
            "q values:  tensor([[0.3406, 0.3850, 0.3958, 0.3732]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 19\n",
            "q values:  tensor([[0.3416, 0.3806, 0.3857, 0.3848]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 20\n",
            "q values:  tensor([[0.3555, 0.3769, 0.3950, 0.3948]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 21\n",
            "q values:  tensor([[0.3544, 0.3674, 0.3954, 0.4129]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 22\n",
            "q values:  tensor([[0.3732, 0.3701, 0.4216, 0.3941]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 23\n",
            "q values:  tensor([[0.3764, 0.3884, 0.4199, 0.3921]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 24\n",
            "q values:  tensor([[0.3824, 0.3764, 0.4126, 0.3958]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 25\n",
            "q values:  tensor([[0.3745, 0.3936, 0.4062, 0.3847]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 26\n",
            "q values:  tensor([[0.3660, 0.3881, 0.4003, 0.3694]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 27\n",
            "q values:  tensor([[0.3638, 0.3714, 0.3950, 0.3730]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 28\n",
            "q values:  tensor([[0.3571, 0.3912, 0.4031, 0.3728]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9835, iteration 29\n",
            "q values:  tensor([[0.3757, 0.3919, 0.4055, 0.3764]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9836, iteration 0\n",
            "q values:  tensor([[0.3730, 0.3869, 0.4049, 0.3668]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9836, iteration 1\n",
            "q values:  tensor([[0.3699, 0.3708, 0.4044, 0.3841]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9836, iteration 2\n",
            "q values:  tensor([[0.3939, 0.3696, 0.3934, 0.3850]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9836, iteration 3\n",
            "q values:  tensor([[0.4022, 0.3868, 0.3891, 0.3807]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9837, iteration 0\n",
            "q values:  tensor([[0.4148, 0.3825, 0.3794, 0.3999]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9837, iteration 1\n",
            "q values:  tensor([[0.4125, 0.3785, 0.3802, 0.3884]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9837, iteration 2\n",
            "q values:  tensor([[0.4111, 0.3749, 0.3899, 0.3840]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9838, iteration 0\n",
            "q values:  tensor([[0.4233, 0.3730, 0.3804, 0.3856]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9838, iteration 1\n",
            "q values:  tensor([[0.4335, 0.3840, 0.3727, 0.3811]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9838, iteration 2\n",
            "q values:  tensor([[0.4248, 0.3831, 0.3703, 0.3827]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9838, iteration 3\n",
            "q values:  tensor([[0.4113, 0.3944, 0.3572, 0.3784]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9839, iteration 0\n",
            "q values:  tensor([[0.3953, 0.3955, 0.3547, 0.3695]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9839, iteration 1\n",
            "q values:  tensor([[0.3854, 0.4055, 0.3464, 0.3937]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9839, iteration 2\n",
            "q values:  tensor([[0.3771, 0.3924, 0.3589, 0.4030]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9839, iteration 3\n",
            "q values:  tensor([[0.3794, 0.3931, 0.3634, 0.3968]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9840, iteration 0\n",
            "q values:  tensor([[0.3680, 0.3936, 0.3589, 0.3748]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9840, iteration 1\n",
            "q values:  tensor([[0.3725, 0.3817, 0.3634, 0.3902]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9840, iteration 2\n",
            "q values:  tensor([[0.3709, 0.3711, 0.3664, 0.3941]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9840, iteration 3\n",
            "q values:  tensor([[0.3680, 0.3608, 0.3757, 0.3890]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9841, iteration 0\n",
            "q values:  tensor([[0.3621, 0.3456, 0.3924, 0.3981]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9841, iteration 1\n",
            "q values:  tensor([[0.3611, 0.3615, 0.3875, 0.3924]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9841, iteration 2\n",
            "q values:  tensor([[0.3583, 0.3542, 0.3781, 0.3872]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9841, iteration 3\n",
            "q values:  tensor([[0.3551, 0.3582, 0.3701, 0.3776]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9842, iteration 0\n",
            "q values:  tensor([[0.3532, 0.3569, 0.3739, 0.3825]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9842, iteration 1\n",
            "q values:  tensor([[0.3479, 0.3702, 0.3710, 0.3673]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9842, iteration 2\n",
            "q values:  tensor([[0.3493, 0.3905, 0.3688, 0.3784]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9843, iteration 0\n",
            "q values:  tensor([[0.3504, 0.4064, 0.3721, 0.3749]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9843, iteration 1\n",
            "q values:  tensor([[0.3407, 0.3995, 0.3697, 0.3659]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9843, iteration 2\n",
            "q values:  tensor([[0.3461, 0.4085, 0.3632, 0.3681]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9843, iteration 3\n",
            "q values:  tensor([[0.3510, 0.4077, 0.3549, 0.3741]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9843, iteration 4\n",
            "q values:  tensor([[0.3519, 0.4215, 0.3579, 0.3594]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9843, iteration 5\n",
            "q values:  tensor([[0.3633, 0.4213, 0.3597, 0.3638]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9843, iteration 6\n",
            "q values:  tensor([[0.3670, 0.4274, 0.3605, 0.3653]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9843, iteration 7\n",
            "q values:  tensor([[0.3616, 0.4061, 0.3764, 0.3690]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9843, iteration 8\n",
            "q values:  tensor([[0.3646, 0.4049, 0.3684, 0.3657]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9843, iteration 9\n",
            "q values:  tensor([[0.3696, 0.3980, 0.3591, 0.3583]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9843, iteration 10\n",
            "q values:  tensor([[0.3664, 0.3980, 0.3476, 0.3565]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9843, iteration 11\n",
            "q values:  tensor([[0.3690, 0.3918, 0.3533, 0.3567]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9843, iteration 12\n",
            "q values:  tensor([[0.3719, 0.3863, 0.3663, 0.3563]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9843, iteration 13\n",
            "q values:  tensor([[0.3677, 0.3754, 0.3669, 0.3482]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9843, iteration 14\n",
            "q values:  tensor([[0.3711, 0.3778, 0.3854, 0.3526]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9843, iteration 15\n",
            "q values:  tensor([[0.3700, 0.3735, 0.4005, 0.3514]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9843, iteration 16\n",
            "q values:  tensor([[0.3735, 0.3695, 0.3836, 0.3481]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9843, iteration 17\n",
            "q values:  tensor([[0.3767, 0.3663, 0.3798, 0.3556]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9844, iteration 0\n",
            "q values:  tensor([[0.3820, 0.3587, 0.3767, 0.3575]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9844, iteration 1\n",
            "q values:  tensor([[0.3783, 0.3555, 0.3924, 0.3462]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9844, iteration 2\n",
            "q values:  tensor([[0.3800, 0.3615, 0.4015, 0.3516]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9844, iteration 3\n",
            "q values:  tensor([[0.3900, 0.3762, 0.4150, 0.3536]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9845, iteration 0\n",
            "q values:  tensor([[0.3738, 0.3608, 0.4020, 0.3713]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9845, iteration 1\n",
            "q values:  tensor([[0.3701, 0.3541, 0.4015, 0.3685]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9845, iteration 2\n",
            "q values:  tensor([[0.3775, 0.3677, 0.3954, 0.3694]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9845, iteration 3\n",
            "q values:  tensor([[0.3709, 0.3638, 0.3901, 0.3611]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9845, iteration 4\n",
            "q values:  tensor([[0.3884, 0.3707, 0.4049, 0.3645]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9845, iteration 5\n",
            "q values:  tensor([[0.3778, 0.3742, 0.3987, 0.3792]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9845, iteration 6\n",
            "q values:  tensor([[0.3686, 0.3701, 0.3930, 0.3806]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9845, iteration 7\n",
            "q values:  tensor([[0.3548, 0.3810, 0.3822, 0.3547]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9845, iteration 8\n",
            "q values:  tensor([[0.3639, 0.3762, 0.3981, 0.3519]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9845, iteration 9\n",
            "q values:  tensor([[0.3693, 0.3819, 0.3925, 0.3515]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9845, iteration 10\n",
            "q values:  tensor([[0.3566, 0.3834, 0.3872, 0.3614]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9845, iteration 11\n",
            "q values:  tensor([[0.3835, 0.3844, 0.4023, 0.3663]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9845, iteration 12\n",
            "q values:  tensor([[0.3789, 0.3738, 0.4017, 0.3636]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9845, iteration 13\n",
            "q values:  tensor([[0.3802, 0.3652, 0.4097, 0.3763]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9845, iteration 14\n",
            "q values:  tensor([[0.3848, 0.3717, 0.4085, 0.3882]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9845, iteration 15\n",
            "q values:  tensor([[0.3752, 0.3895, 0.4018, 0.3838]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9845, iteration 16\n",
            "q values:  tensor([[0.3718, 0.4063, 0.3956, 0.3793]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9845, iteration 17\n",
            "q values:  tensor([[0.3635, 0.4081, 0.3902, 0.3892]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9846, iteration 0\n",
            "q values:  tensor([[0.3674, 0.4099, 0.3852, 0.3843]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9846, iteration 1\n",
            "q values:  tensor([[0.3841, 0.4328, 0.3862, 0.3677]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9846, iteration 2\n",
            "q values:  tensor([[0.3642, 0.4237, 0.4098, 0.3705]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9846, iteration 3\n",
            "q values:  tensor([[0.3670, 0.4217, 0.4083, 0.3754]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9847, iteration 0\n",
            "q values:  tensor([[0.3657, 0.4050, 0.4101, 0.3768]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9847, iteration 1\n",
            "q values:  tensor([[0.3782, 0.4071, 0.4092, 0.3879]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9847, iteration 2\n",
            "q values:  tensor([[0.3798, 0.3936, 0.4221, 0.3774]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9847, iteration 3\n",
            "q values:  tensor([[0.4084, 0.3940, 0.4074, 0.4029]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9847, iteration 4\n",
            "q values:  tensor([[0.4040, 0.3944, 0.4010, 0.3966]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9847, iteration 5\n",
            "q values:  tensor([[0.3925, 0.4106, 0.3950, 0.3907]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9847, iteration 6\n",
            "q values:  tensor([[0.4013, 0.4084, 0.3896, 0.4058]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9847, iteration 7\n",
            "q values:  tensor([[0.3848, 0.4097, 0.3844, 0.3987]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9847, iteration 8\n",
            "q values:  tensor([[0.3862, 0.3949, 0.3881, 0.3986]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9847, iteration 9\n",
            "q values:  tensor([[0.3875, 0.3889, 0.3921, 0.3866]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9847, iteration 10\n",
            "q values:  tensor([[0.3886, 0.3834, 0.3959, 0.3878]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9847, iteration 11\n",
            "q values:  tensor([[0.3784, 0.3878, 0.3960, 0.3929]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9847, iteration 12\n",
            "q values:  tensor([[0.3743, 0.3891, 0.3847, 0.3877]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9848, iteration 0\n",
            "q values:  tensor([[0.3698, 0.3902, 0.3748, 0.3835]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9848, iteration 1\n",
            "q values:  tensor([[0.3464, 0.4072, 0.4043, 0.3787]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9848, iteration 2\n",
            "q values:  tensor([[0.3365, 0.3996, 0.3871, 0.3627]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9848, iteration 3\n",
            "q values:  tensor([[0.3406, 0.3866, 0.3966, 0.3604]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9849, iteration 0\n",
            "q values:  tensor([[0.3345, 0.3816, 0.3967, 0.3540]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9849, iteration 1\n",
            "q values:  tensor([[0.3297, 0.3995, 0.3968, 0.3613]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9849, iteration 2\n",
            "q values:  tensor([[0.3434, 0.3805, 0.3969, 0.3574]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9850, iteration 0\n",
            "q values:  tensor([[0.3493, 0.3702, 0.4109, 0.3496]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9850, iteration 1\n",
            "q values:  tensor([[0.3546, 0.3608, 0.4083, 0.3518]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9850, iteration 2\n",
            "q values:  tensor([[0.3550, 0.3575, 0.4015, 0.3520]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9850, iteration 3\n",
            "q values:  tensor([[0.3531, 0.3607, 0.4151, 0.3487]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9850, iteration 4\n",
            "q values:  tensor([[0.3722, 0.3591, 0.4023, 0.3685]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9850, iteration 5\n",
            "q values:  tensor([[0.3881, 0.3577, 0.3802, 0.3871]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9850, iteration 6\n",
            "q values:  tensor([[0.3894, 0.3596, 0.3714, 0.4120]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9850, iteration 7\n",
            "q values:  tensor([[0.3850, 0.3639, 0.3685, 0.4131]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9850, iteration 8\n",
            "q values:  tensor([[0.3997, 0.3875, 0.3646, 0.3993]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9850, iteration 9\n",
            "q values:  tensor([[0.3887, 0.4040, 0.3618, 0.4021]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9850, iteration 10\n",
            "q values:  tensor([[0.4029, 0.4033, 0.3677, 0.4161]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9850, iteration 11\n",
            "q values:  tensor([[0.3863, 0.4029, 0.3599, 0.3964]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9850, iteration 12\n",
            "q values:  tensor([[0.3954, 0.3960, 0.3631, 0.3908]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9851, iteration 0\n",
            "q values:  tensor([[0.3903, 0.3994, 0.3595, 0.4005]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9851, iteration 1\n",
            "q values:  tensor([[0.3994, 0.4090, 0.3578, 0.3889]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9851, iteration 2\n",
            "q values:  tensor([[0.3941, 0.4170, 0.3451, 0.3842]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9852, iteration 0\n",
            "q values:  tensor([[0.3896, 0.4082, 0.3606, 0.3890]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9852, iteration 1\n",
            "q values:  tensor([[0.3805, 0.4039, 0.3634, 0.3841]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9852, iteration 2\n",
            "q values:  tensor([[0.3823, 0.3905, 0.3799, 0.3887]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9853, iteration 0\n",
            "q values:  tensor([[0.3591, 0.3849, 0.3763, 0.4042]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9853, iteration 1\n",
            "q values:  tensor([[0.3656, 0.4016, 0.3909, 0.3918]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9853, iteration 2\n",
            "q values:  tensor([[0.3490, 0.3889, 0.4006, 0.3864]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9853, iteration 3\n",
            "q values:  tensor([[0.3524, 0.3840, 0.3978, 0.3972]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9854, iteration 0\n",
            "q values:  tensor([[0.3556, 0.3675, 0.3869, 0.3917]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9854, iteration 1\n",
            "q values:  tensor([[0.3538, 0.3721, 0.3783, 0.3921]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9854, iteration 2\n",
            "q values:  tensor([[0.3555, 0.3915, 0.3689, 0.3754]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9854, iteration 3\n",
            "q values:  tensor([[0.3519, 0.3803, 0.3716, 0.3773]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9854, iteration 4\n",
            "q values:  tensor([[0.3450, 0.4007, 0.3631, 0.3688]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9854, iteration 5\n",
            "q values:  tensor([[0.3463, 0.4006, 0.3616, 0.3712]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9854, iteration 6\n",
            "q values:  tensor([[0.3500, 0.3944, 0.3541, 0.3839]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9854, iteration 7\n",
            "q values:  tensor([[0.3507, 0.3952, 0.3600, 0.3791]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9854, iteration 8\n",
            "q values:  tensor([[0.3630, 0.3835, 0.3549, 0.3810]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9854, iteration 9\n",
            "q values:  tensor([[0.3621, 0.4035, 0.3836, 0.3839]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9854, iteration 10\n",
            "q values:  tensor([[0.3498, 0.4183, 0.3843, 0.3818]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9854, iteration 11\n",
            "q values:  tensor([[0.3539, 0.4038, 0.3882, 0.3611]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9854, iteration 12\n",
            "q values:  tensor([[0.3467, 0.3913, 0.3839, 0.3652]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9854, iteration 13\n",
            "q values:  tensor([[0.3389, 0.3862, 0.3858, 0.3618]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9854, iteration 14\n",
            "q values:  tensor([[0.3338, 0.4114, 0.3760, 0.3658]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9854, iteration 15\n",
            "q values:  tensor([[0.3347, 0.4193, 0.3734, 0.3856]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9854, iteration 16\n",
            "q values:  tensor([[0.3666, 0.4141, 0.3896, 0.3962]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9854, iteration 17\n",
            "q values:  tensor([[0.3529, 0.4004, 0.3796, 0.3908]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9854, iteration 18\n",
            "q values:  tensor([[0.3485, 0.4004, 0.3729, 0.3862]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9854, iteration 19\n",
            "q values:  tensor([[0.3433, 0.3886, 0.3843, 0.3815]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9855, iteration 0\n",
            "q values:  tensor([[0.3372, 0.3841, 0.3754, 0.3865]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9855, iteration 1\n",
            "q values:  tensor([[0.3421, 0.3953, 0.3796, 0.4023]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9855, iteration 2\n",
            "q values:  tensor([[0.3710, 0.3844, 0.3765, 0.3962]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9855, iteration 3\n",
            "q values:  tensor([[0.3629, 0.3799, 0.3689, 0.3908]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9856, iteration 0\n",
            "q values:  tensor([[0.3564, 0.3705, 0.3615, 0.3917]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9856, iteration 1\n",
            "q values:  tensor([[0.3522, 0.3722, 0.3608, 0.3867]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9856, iteration 2\n",
            "q values:  tensor([[0.3486, 0.3633, 0.3653, 0.4110]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9856, iteration 3\n",
            "q values:  tensor([[0.3625, 0.3765, 0.3829, 0.4102]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9856, iteration 4\n",
            "q values:  tensor([[0.3771, 0.3819, 0.3836, 0.4093]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9856, iteration 5\n",
            "q values:  tensor([[0.3743, 0.3843, 0.3862, 0.4028]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9856, iteration 6\n",
            "q values:  tensor([[0.3796, 0.3954, 0.3819, 0.4174]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9856, iteration 7\n",
            "q values:  tensor([[0.3894, 0.4049, 0.3780, 0.4441]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 8\n",
            "q values:  tensor([[0.3855, 0.4329, 0.3680, 0.4340]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 9\n",
            "q values:  tensor([[0.3869, 0.4296, 0.3620, 0.4253]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 10\n",
            "q values:  tensor([[0.3966, 0.4196, 0.3642, 0.4371]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 11\n",
            "q values:  tensor([[0.3971, 0.4317, 0.3674, 0.4282]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 12\n",
            "q values:  tensor([[0.4232, 0.4112, 0.3727, 0.4192]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 13\n",
            "q values:  tensor([[0.4459, 0.3933, 0.3696, 0.4110]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 14\n",
            "q values:  tensor([[0.4362, 0.3828, 0.3669, 0.4037]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 15\n",
            "q values:  tensor([[0.4163, 0.3796, 0.3629, 0.4177]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 16\n",
            "q values:  tensor([[0.4101, 0.3763, 0.3631, 0.4045]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 17\n",
            "q values:  tensor([[0.4040, 0.3871, 0.3769, 0.4042]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 18\n",
            "q values:  tensor([[0.4037, 0.3884, 0.3748, 0.4037]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 19\n",
            "q values:  tensor([[0.4034, 0.3985, 0.3666, 0.4005]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 20\n",
            "q values:  tensor([[0.3979, 0.3930, 0.3626, 0.3946]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 21\n",
            "q values:  tensor([[0.3982, 0.3878, 0.3772, 0.3952]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 22\n",
            "q values:  tensor([[0.3932, 0.3833, 0.3803, 0.4046]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 23\n",
            "q values:  tensor([[0.3887, 0.3684, 0.3812, 0.3864]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9856, iteration 24\n",
            "q values:  tensor([[0.3800, 0.3588, 0.3829, 0.3913]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 25\n",
            "q values:  tensor([[0.3818, 0.3623, 0.3994, 0.3812]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 26\n",
            "q values:  tensor([[0.3748, 0.3640, 0.3995, 0.3773]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 27\n",
            "q values:  tensor([[0.3771, 0.3815, 0.4145, 0.3735]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 28\n",
            "q values:  tensor([[0.3645, 0.3777, 0.4066, 0.3899]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 29\n",
            "q values:  tensor([[0.3824, 0.3682, 0.3939, 0.3995]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 30\n",
            "q values:  tensor([[0.3849, 0.3538, 0.3888, 0.4140]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 31\n",
            "q values:  tensor([[0.3857, 0.3510, 0.3845, 0.4130]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 32\n",
            "q values:  tensor([[0.3772, 0.3531, 0.3801, 0.4195]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 33\n",
            "q values:  tensor([[0.3825, 0.3463, 0.3698, 0.4111]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 34\n",
            "q values:  tensor([[0.3739, 0.3502, 0.3779, 0.4100]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 35\n",
            "q values:  tensor([[0.3665, 0.3444, 0.3962, 0.4094]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 36\n",
            "q values:  tensor([[0.3595, 0.3286, 0.3966, 0.4024]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 37\n",
            "q values:  tensor([[0.3569, 0.3311, 0.4259, 0.3962]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 38\n",
            "q values:  tensor([[0.3463, 0.3512, 0.4251, 0.3906]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 39\n",
            "q values:  tensor([[0.3642, 0.3582, 0.4156, 0.3999]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9856, iteration 40\n",
            "q values:  tensor([[0.3618, 0.3776, 0.4080, 0.4147]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 41\n",
            "q values:  tensor([[0.3792, 0.3692, 0.4302, 0.4132]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 42\n",
            "q values:  tensor([[0.3931, 0.3614, 0.4214, 0.4109]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 43\n",
            "q values:  tensor([[0.3887, 0.3555, 0.4079, 0.4097]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 44\n",
            "q values:  tensor([[0.3794, 0.3562, 0.4157, 0.4176]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 45\n",
            "q values:  tensor([[0.3813, 0.3491, 0.4078, 0.4095]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 46\n",
            "q values:  tensor([[0.3832, 0.3485, 0.4011, 0.4173]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 47\n",
            "q values:  tensor([[0.3741, 0.3495, 0.3981, 0.4119]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 48\n",
            "q values:  tensor([[0.3801, 0.3488, 0.3869, 0.4051]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 49\n",
            "q values:  tensor([[0.3715, 0.3490, 0.3884, 0.3983]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 50\n",
            "q values:  tensor([[0.3681, 0.3491, 0.3732, 0.3924]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9856, iteration 51\n",
            "q values:  tensor([[0.3702, 0.3433, 0.3709, 0.3875]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  3.0\n",
            "episode 9857, iteration 0\n",
            "q values:  tensor([[0.3669, 0.3365, 0.3681, 0.3978]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9857, iteration 1\n",
            "q values:  tensor([[0.3779, 0.3291, 0.3646, 0.3920]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9857, iteration 2\n",
            "q values:  tensor([[0.3650, 0.3454, 0.3672, 0.4017]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9858, iteration 0\n",
            "q values:  tensor([[0.3639, 0.3278, 0.3975, 0.4107]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9858, iteration 1\n",
            "q values:  tensor([[0.3624, 0.3521, 0.3977, 0.4246]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9858, iteration 2\n",
            "q values:  tensor([[0.3611, 0.3562, 0.3980, 0.4208]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9858, iteration 3\n",
            "q values:  tensor([[0.3555, 0.3442, 0.3925, 0.4127]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9859, iteration 0\n",
            "q values:  tensor([[0.3436, 0.3408, 0.4077, 0.4058]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9859, iteration 1\n",
            "q values:  tensor([[0.3365, 0.3495, 0.4154, 0.3991]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9859, iteration 2\n",
            "q values:  tensor([[0.3380, 0.3503, 0.3970, 0.4146]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9860, iteration 0\n",
            "q values:  tensor([[0.3203, 0.3474, 0.3808, 0.4008]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9860, iteration 1\n",
            "q values:  tensor([[0.3149, 0.3435, 0.3816, 0.3885]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9860, iteration 2\n",
            "q values:  tensor([[0.3268, 0.3393, 0.3837, 0.3896]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9860, iteration 3\n",
            "q values:  tensor([[0.3383, 0.3405, 0.3799, 0.3996]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9861, iteration 0\n",
            "q values:  tensor([[0.3441, 0.3425, 0.3895, 0.3935]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9861, iteration 1\n",
            "q values:  tensor([[0.3441, 0.3740, 0.3874, 0.3935]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9861, iteration 2\n",
            "q values:  tensor([[0.3569, 0.3767, 0.4151, 0.3873]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9861, iteration 3\n",
            "q values:  tensor([[0.3723, 0.3732, 0.4139, 0.3699]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9862, iteration 0\n",
            "q values:  tensor([[0.3690, 0.3592, 0.4125, 0.3558]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9862, iteration 1\n",
            "q values:  tensor([[0.3843, 0.3524, 0.3993, 0.3538]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9862, iteration 2\n",
            "q values:  tensor([[0.3799, 0.3515, 0.3878, 0.3520]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9862, iteration 3\n",
            "q values:  tensor([[0.3895, 0.3544, 0.3885, 0.3449]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9862, iteration 4\n",
            "q values:  tensor([[0.3743, 0.3577, 0.3836, 0.3433]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9862, iteration 5\n",
            "q values:  tensor([[0.3663, 0.3628, 0.4062, 0.3464]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9862, iteration 6\n",
            "q values:  tensor([[0.3681, 0.3554, 0.4051, 0.3307]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9862, iteration 7\n",
            "q values:  tensor([[0.3662, 0.3598, 0.4175, 0.3594]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9862, iteration 8\n",
            "q values:  tensor([[0.3680, 0.3651, 0.4149, 0.3634]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 9\n",
            "q values:  tensor([[0.3825, 0.3521, 0.4205, 0.3556]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 10\n",
            "q values:  tensor([[0.3783, 0.3524, 0.4117, 0.3584]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 11\n",
            "q values:  tensor([[0.3802, 0.3553, 0.3992, 0.3583]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 12\n",
            "q values:  tensor([[0.3757, 0.3540, 0.4064, 0.3696]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 13\n",
            "q values:  tensor([[0.3764, 0.3594, 0.3999, 0.3724]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 14\n",
            "q values:  tensor([[0.4046, 0.3596, 0.4132, 0.3683]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 15\n",
            "q values:  tensor([[0.3778, 0.3592, 0.4252, 0.3699]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 16\n",
            "q values:  tensor([[0.3689, 0.3638, 0.4357, 0.3824]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 17\n",
            "q values:  tensor([[0.3789, 0.3556, 0.4194, 0.3833]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 18\n",
            "q values:  tensor([[0.3747, 0.3458, 0.4158, 0.3989]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9862, iteration 19\n",
            "q values:  tensor([[0.3778, 0.3536, 0.4086, 0.3987]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9863, iteration 0\n",
            "q values:  tensor([[0.3925, 0.3536, 0.4075, 0.3981]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9863, iteration 1\n",
            "q values:  tensor([[0.3875, 0.3590, 0.4196, 0.3917]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9863, iteration 2\n",
            "q values:  tensor([[0.3987, 0.3582, 0.4071, 0.3918]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9863, iteration 3\n",
            "q values:  tensor([[0.3880, 0.3619, 0.3951, 0.3739]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9864, iteration 0\n",
            "q values:  tensor([[0.3884, 0.3671, 0.3949, 0.3644]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9864, iteration 1\n",
            "q values:  tensor([[0.3641, 0.3788, 0.3894, 0.3777]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9864, iteration 2\n",
            "q values:  tensor([[0.3522, 0.3895, 0.3898, 0.3796]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9864, iteration 3\n",
            "q values:  tensor([[0.3459, 0.3782, 0.3847, 0.3750]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9865, iteration 0\n",
            "q values:  tensor([[0.3848, 0.3731, 0.3747, 0.3871]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9865, iteration 1\n",
            "q values:  tensor([[0.3926, 0.3692, 0.3607, 0.3879]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9865, iteration 2\n",
            "q values:  tensor([[0.3881, 0.3658, 0.3530, 0.3884]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9866, iteration 0\n",
            "q values:  tensor([[0.3793, 0.3618, 0.3543, 0.3982]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9866, iteration 1\n",
            "q values:  tensor([[0.3760, 0.3642, 0.3547, 0.3914]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9866, iteration 2\n",
            "q values:  tensor([[0.3759, 0.3822, 0.3405, 0.3855]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9867, iteration 0\n",
            "q values:  tensor([[0.3684, 0.3656, 0.3592, 0.3802]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9867, iteration 1\n",
            "q values:  tensor([[0.3625, 0.3558, 0.3513, 0.3758]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9867, iteration 2\n",
            "q values:  tensor([[0.3879, 0.3736, 0.3570, 0.3873]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9868, iteration 0\n",
            "q values:  tensor([[0.3882, 0.3789, 0.3576, 0.3756]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9868, iteration 1\n",
            "q values:  tensor([[0.3884, 0.3749, 0.3765, 0.3589]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9868, iteration 2\n",
            "q values:  tensor([[0.3843, 0.3857, 0.3736, 0.3710]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9869, iteration 0\n",
            "q values:  tensor([[0.3876, 0.3865, 0.3706, 0.3825]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9869, iteration 1\n",
            "q values:  tensor([[0.3880, 0.4022, 0.3761, 0.3843]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9869, iteration 2\n",
            "q values:  tensor([[0.3836, 0.3892, 0.3773, 0.3563]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9870, iteration 0\n",
            "q values:  tensor([[0.3843, 0.3896, 0.3770, 0.3542]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9870, iteration 1\n",
            "q values:  tensor([[0.3848, 0.4050, 0.3877, 0.3590]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9870, iteration 2\n",
            "q values:  tensor([[0.3930, 0.3803, 0.3826, 0.3776]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9870, iteration 3\n",
            "q values:  tensor([[0.4004, 0.3814, 0.3781, 0.3945]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9871, iteration 0\n",
            "q values:  tensor([[0.3806, 0.3761, 0.3885, 0.3885]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9871, iteration 1\n",
            "q values:  tensor([[0.3721, 0.3657, 0.3830, 0.3826]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9871, iteration 2\n",
            "q values:  tensor([[0.3687, 0.3684, 0.3835, 0.3719]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9872, iteration 0\n",
            "q values:  tensor([[0.3617, 0.3713, 0.3784, 0.3615]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9872, iteration 1\n",
            "q values:  tensor([[0.3605, 0.3510, 0.3934, 0.3718]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9872, iteration 2\n",
            "q values:  tensor([[0.3531, 0.3638, 0.3873, 0.3776]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9873, iteration 0\n",
            "q values:  tensor([[0.3540, 0.3919, 0.3906, 0.3557]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9873, iteration 1\n",
            "q values:  tensor([[0.3660, 0.3700, 0.3739, 0.3571]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9873, iteration 2\n",
            "q values:  tensor([[0.3761, 0.3659, 0.3596, 0.3599]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9873, iteration 3\n",
            "q values:  tensor([[0.3770, 0.3630, 0.3648, 0.3649]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9874, iteration 0\n",
            "q values:  tensor([[0.3781, 0.3640, 0.3897, 0.3626]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9874, iteration 1\n",
            "q values:  tensor([[0.3861, 0.3521, 0.3896, 0.3656]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9874, iteration 2\n",
            "q values:  tensor([[0.3862, 0.3685, 0.3840, 0.3625]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9875, iteration 0\n",
            "q values:  tensor([[0.3864, 0.3712, 0.3791, 0.3641]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9875, iteration 1\n",
            "q values:  tensor([[0.3943, 0.3722, 0.4016, 0.3563]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9875, iteration 2\n",
            "q values:  tensor([[0.3887, 0.3878, 0.3898, 0.3742]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9875, iteration 3\n",
            "q values:  tensor([[0.3695, 0.3878, 0.3791, 0.3698]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9876, iteration 0\n",
            "q values:  tensor([[0.3656, 0.4017, 0.3885, 0.3726]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9876, iteration 1\n",
            "q values:  tensor([[0.3579, 0.4004, 0.3884, 0.3738]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9876, iteration 2\n",
            "q values:  tensor([[0.3689, 0.3939, 0.3831, 0.3639]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9876, iteration 3\n",
            "q values:  tensor([[0.3662, 0.3772, 0.3781, 0.3557]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9877, iteration 0\n",
            "q values:  tensor([[0.3684, 0.3728, 0.3741, 0.3473]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9877, iteration 1\n",
            "q values:  tensor([[0.3650, 0.3689, 0.3757, 0.3685]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9877, iteration 2\n",
            "q values:  tensor([[0.3753, 0.3793, 0.3855, 0.3845]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9877, iteration 3\n",
            "q values:  tensor([[0.3763, 0.3697, 0.3805, 0.3791]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9878, iteration 0\n",
            "q values:  tensor([[0.3773, 0.3508, 0.3758, 0.3949]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9878, iteration 1\n",
            "q values:  tensor([[0.3780, 0.3404, 0.3801, 0.4032]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9878, iteration 2\n",
            "q values:  tensor([[0.3874, 0.3377, 0.3948, 0.4164]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9878, iteration 3\n",
            "q values:  tensor([[0.3874, 0.3510, 0.4159, 0.4222]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9878, iteration 4\n",
            "q values:  tensor([[0.3825, 0.3677, 0.4071, 0.4062]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9878, iteration 5\n",
            "q values:  tensor([[0.3780, 0.3649, 0.4005, 0.4032]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9878, iteration 6\n",
            "q values:  tensor([[0.3690, 0.3568, 0.3995, 0.3787]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9878, iteration 7\n",
            "q values:  tensor([[0.3657, 0.3536, 0.3823, 0.3803]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9878, iteration 8\n",
            "q values:  tensor([[0.3638, 0.3772, 0.3671, 0.3700]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9878, iteration 9\n",
            "q values:  tensor([[0.3515, 0.3728, 0.3539, 0.3608]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9878, iteration 10\n",
            "q values:  tensor([[0.3668, 0.3694, 0.3554, 0.3583]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9878, iteration 11\n",
            "q values:  tensor([[0.3637, 0.3656, 0.3497, 0.3703]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9878, iteration 12\n",
            "q values:  tensor([[0.3659, 0.3718, 0.3612, 0.3664]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9878, iteration 13\n",
            "q values:  tensor([[0.3674, 0.3745, 0.3717, 0.3633]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9878, iteration 14\n",
            "q values:  tensor([[0.3634, 0.3706, 0.3763, 0.3706]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9878, iteration 15\n",
            "q values:  tensor([[0.3740, 0.3865, 0.3720, 0.3667]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9878, iteration 16\n",
            "q values:  tensor([[0.3886, 0.3813, 0.3695, 0.3622]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9879, iteration 0\n",
            "q values:  tensor([[0.3784, 0.3907, 0.3705, 0.3668]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9879, iteration 1\n",
            "q values:  tensor([[0.3699, 0.3797, 0.3671, 0.3678]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9879, iteration 2\n",
            "q values:  tensor([[0.3667, 0.3806, 0.3589, 0.3730]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9879, iteration 3\n",
            "q values:  tensor([[0.3762, 0.3901, 0.3517, 0.3685]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9879, iteration 4\n",
            "q values:  tensor([[0.3713, 0.3844, 0.3493, 0.3693]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9879, iteration 5\n",
            "q values:  tensor([[0.3733, 0.3794, 0.3622, 0.3713]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9879, iteration 6\n",
            "q values:  tensor([[0.3777, 0.3750, 0.3864, 0.3731]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9879, iteration 7\n",
            "q values:  tensor([[0.3820, 0.3763, 0.3865, 0.3786]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9879, iteration 8\n",
            "q values:  tensor([[0.3825, 0.3776, 0.4003, 0.3893]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9879, iteration 9\n",
            "q values:  tensor([[0.3679, 0.3784, 0.3882, 0.4044]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9879, iteration 10\n",
            "q values:  tensor([[0.3649, 0.3684, 0.3882, 0.3909]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9879, iteration 11\n",
            "q values:  tensor([[0.3664, 0.3558, 0.3828, 0.3847]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9879, iteration 12\n",
            "q values:  tensor([[0.3632, 0.3713, 0.3782, 0.3621]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9880, iteration 0\n",
            "q values:  tensor([[0.3663, 0.3682, 0.3739, 0.3582]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9880, iteration 1\n",
            "q values:  tensor([[0.3810, 0.3704, 0.3648, 0.3774]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9880, iteration 2\n",
            "q values:  tensor([[0.3799, 0.3662, 0.3597, 0.3927]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9881, iteration 0\n",
            "q values:  tensor([[0.3755, 0.3816, 0.3578, 0.3807]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9881, iteration 1\n",
            "q values:  tensor([[0.3793, 0.3956, 0.3544, 0.3953]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9881, iteration 2\n",
            "q values:  tensor([[0.3881, 0.3882, 0.3506, 0.3770]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9882, iteration 0\n",
            "q values:  tensor([[0.4012, 0.4013, 0.3491, 0.3774]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9882, iteration 1\n",
            "q values:  tensor([[0.3998, 0.4021, 0.3663, 0.3670]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9882, iteration 2\n",
            "q values:  tensor([[0.4005, 0.3988, 0.3621, 0.3625]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9883, iteration 0\n",
            "q values:  tensor([[0.3879, 0.3889, 0.3527, 0.3681]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9883, iteration 1\n",
            "q values:  tensor([[0.3763, 0.4024, 0.3654, 0.3687]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9883, iteration 2\n",
            "q values:  tensor([[0.3766, 0.4000, 0.3763, 0.3702]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9883, iteration 3\n",
            "q values:  tensor([[0.3515, 0.3992, 0.3856, 0.3751]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9884, iteration 0\n",
            "q values:  tensor([[0.3658, 0.4057, 0.3831, 0.3756]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9884, iteration 1\n",
            "q values:  tensor([[0.3670, 0.3920, 0.3774, 0.3649]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9884, iteration 2\n",
            "q values:  tensor([[0.3579, 0.3911, 0.3723, 0.3858]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9884, iteration 3\n",
            "q values:  tensor([[0.3498, 0.3984, 0.3822, 0.3855]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9885, iteration 0\n",
            "q values:  tensor([[0.3382, 0.3907, 0.3713, 0.3740]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9885, iteration 1\n",
            "q values:  tensor([[0.3504, 0.3789, 0.3863, 0.3636]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9885, iteration 2\n",
            "q values:  tensor([[0.3541, 0.3737, 0.3748, 0.3643]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9885, iteration 3\n",
            "q values:  tensor([[0.3595, 0.3744, 0.3978, 0.3808]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9885, iteration 4\n",
            "q values:  tensor([[0.3524, 0.3640, 0.4233, 0.3698]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9885, iteration 5\n",
            "q values:  tensor([[0.3580, 0.3757, 0.4265, 0.3793]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9885, iteration 6\n",
            "q values:  tensor([[0.3872, 0.3708, 0.4023, 0.3739]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9885, iteration 7\n",
            "q values:  tensor([[0.3819, 0.3812, 0.3945, 0.3582]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9885, iteration 8\n",
            "q values:  tensor([[0.3621, 0.3701, 0.3931, 0.3494]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 9\n",
            "q values:  tensor([[0.3679, 0.3657, 0.3868, 0.3518]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 10\n",
            "q values:  tensor([[0.3550, 0.3615, 0.3994, 0.3543]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 11\n",
            "q values:  tensor([[0.3371, 0.3540, 0.3917, 0.3555]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 12\n",
            "q values:  tensor([[0.3411, 0.3528, 0.3806, 0.3526]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 13\n",
            "q values:  tensor([[0.3388, 0.3458, 0.3838, 0.3503]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 14\n",
            "q values:  tensor([[0.3388, 0.3448, 0.3968, 0.3622]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 15\n",
            "q values:  tensor([[0.3322, 0.3424, 0.3895, 0.3534]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 16\n",
            "q values:  tensor([[0.3373, 0.3413, 0.3787, 0.3607]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 17\n",
            "q values:  tensor([[0.3402, 0.3398, 0.3873, 0.3518]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 18\n",
            "q values:  tensor([[0.3434, 0.3521, 0.3867, 0.3535]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9885, iteration 19\n",
            "q values:  tensor([[0.3343, 0.3700, 0.3865, 0.3573]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9886, iteration 0\n",
            "q values:  tensor([[0.3627, 0.3708, 0.3755, 0.3490]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9886, iteration 1\n",
            "q values:  tensor([[0.3641, 0.3662, 0.3660, 0.3534]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9887, iteration 0\n",
            "q values:  tensor([[0.3563, 0.3675, 0.3762, 0.3653]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9887, iteration 1\n",
            "q values:  tensor([[0.3698, 0.3634, 0.3766, 0.3557]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9887, iteration 2\n",
            "q values:  tensor([[0.3661, 0.3530, 0.3720, 0.3427]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9887, iteration 3\n",
            "q values:  tensor([[0.3675, 0.3392, 0.3732, 0.3353]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9888, iteration 0\n",
            "q values:  tensor([[0.3755, 0.3391, 0.3685, 0.3394]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9888, iteration 1\n",
            "q values:  tensor([[0.3793, 0.3359, 0.3639, 0.3562]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9888, iteration 2\n",
            "q values:  tensor([[0.3796, 0.3298, 0.3691, 0.3470]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9888, iteration 3\n",
            "q values:  tensor([[0.3796, 0.3287, 0.3554, 0.3407]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9889, iteration 0\n",
            "q values:  tensor([[0.3699, 0.3293, 0.3520, 0.3462]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9889, iteration 1\n",
            "q values:  tensor([[0.3792, 0.3250, 0.3641, 0.3405]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9889, iteration 2\n",
            "q values:  tensor([[0.3744, 0.3252, 0.3552, 0.3461]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9890, iteration 0\n",
            "q values:  tensor([[0.3748, 0.3161, 0.3745, 0.3509]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9890, iteration 1\n",
            "q values:  tensor([[0.3835, 0.3103, 0.3785, 0.3635]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9890, iteration 2\n",
            "q values:  tensor([[0.3783, 0.3110, 0.3738, 0.3599]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9890, iteration 3\n",
            "q values:  tensor([[0.3917, 0.3339, 0.3694, 0.3570]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9891, iteration 0\n",
            "q values:  tensor([[0.3904, 0.3382, 0.3604, 0.3482]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9891, iteration 1\n",
            "q values:  tensor([[0.3829, 0.3407, 0.3625, 0.3708]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9891, iteration 2\n",
            "q values:  tensor([[0.3783, 0.3438, 0.3574, 0.3814]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9892, iteration 0\n",
            "q values:  tensor([[0.3748, 0.3568, 0.3755, 0.3968]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9892, iteration 1\n",
            "q values:  tensor([[0.3902, 0.3539, 0.3718, 0.3902]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9892, iteration 2\n",
            "q values:  tensor([[0.3849, 0.3753, 0.3881, 0.3843]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9893, iteration 0\n",
            "q values:  tensor([[0.3853, 0.3711, 0.3830, 0.3794]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9893, iteration 1\n",
            "q values:  tensor([[0.3703, 0.3726, 0.3917, 0.3815]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9893, iteration 2\n",
            "q values:  tensor([[0.3630, 0.3889, 0.3861, 0.3702]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9893, iteration 3\n",
            "q values:  tensor([[0.3558, 0.4043, 0.3811, 0.3664]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9894, iteration 0\n",
            "q values:  tensor([[0.3538, 0.4023, 0.3769, 0.3775]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9894, iteration 1\n",
            "q values:  tensor([[0.3501, 0.3958, 0.3624, 0.3727]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9894, iteration 2\n",
            "q values:  tensor([[0.3438, 0.3951, 0.3776, 0.3608]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9895, iteration 0\n",
            "q values:  tensor([[0.3401, 0.3948, 0.3875, 0.3585]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9895, iteration 1\n",
            "q values:  tensor([[0.3449, 0.3915, 0.3825, 0.3489]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9895, iteration 2\n",
            "q values:  tensor([[0.3420, 0.3887, 0.3921, 0.3561]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9895, iteration 3\n",
            "q values:  tensor([[0.3497, 0.3888, 0.3949, 0.3478]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9896, iteration 0\n",
            "q values:  tensor([[0.3482, 0.3829, 0.3890, 0.3470]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9896, iteration 1\n",
            "q values:  tensor([[0.3601, 0.3994, 0.3837, 0.3449]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9896, iteration 2\n",
            "q values:  tensor([[0.3639, 0.3920, 0.3790, 0.3370]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9897, iteration 0\n",
            "q values:  tensor([[0.3606, 0.3856, 0.3802, 0.3376]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9897, iteration 1\n",
            "q values:  tensor([[0.3492, 0.3860, 0.3808, 0.3593]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9897, iteration 2\n",
            "q values:  tensor([[0.3483, 0.3804, 0.3763, 0.3571]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9897, iteration 3\n",
            "q values:  tensor([[0.3378, 0.3815, 0.3867, 0.3685]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9898, iteration 0\n",
            "q values:  tensor([[0.3404, 0.3823, 0.3869, 0.3482]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9898, iteration 1\n",
            "q values:  tensor([[0.3460, 0.3705, 0.3818, 0.3480]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9898, iteration 2\n",
            "q values:  tensor([[0.3777, 0.3671, 0.3714, 0.3552]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9899, iteration 0\n",
            "q values:  tensor([[0.3637, 0.3693, 0.3741, 0.3863]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9899, iteration 1\n",
            "q values:  tensor([[0.3567, 0.3657, 0.3757, 0.3698]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9899, iteration 2\n",
            "q values:  tensor([[0.3560, 0.3625, 0.3719, 0.3656]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9899, iteration 3\n",
            "q values:  tensor([[0.3573, 0.3539, 0.3523, 0.3578]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9899, iteration 4\n",
            "q values:  tensor([[0.3691, 0.3889, 0.3499, 0.3498]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9899, iteration 5\n",
            "q values:  tensor([[0.3611, 0.3830, 0.3466, 0.3449]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9899, iteration 6\n",
            "q values:  tensor([[0.3566, 0.3779, 0.3516, 0.3495]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9899, iteration 7\n",
            "q values:  tensor([[0.3681, 0.3888, 0.3648, 0.3416]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 8\n",
            "q values:  tensor([[0.3797, 0.3829, 0.3614, 0.3466]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 9\n",
            "q values:  tensor([[0.3705, 0.3840, 0.3635, 0.3511]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 10\n",
            "q values:  tensor([[0.3617, 0.3823, 0.3560, 0.3534]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 11\n",
            "q values:  tensor([[0.3591, 0.3985, 0.3683, 0.3531]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 12\n",
            "q values:  tensor([[0.3567, 0.3974, 0.3658, 0.3711]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 13\n",
            "q values:  tensor([[0.3537, 0.3904, 0.3640, 0.3618]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 14\n",
            "q values:  tensor([[0.3698, 0.3782, 0.3605, 0.3540]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 15\n",
            "q values:  tensor([[0.3720, 0.3613, 0.3583, 0.3478]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 16\n",
            "q values:  tensor([[0.3819, 0.3821, 0.3495, 0.3510]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 17\n",
            "q values:  tensor([[0.3827, 0.3919, 0.3434, 0.3537]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9899, iteration 18\n",
            "q values:  tensor([[0.3778, 0.3857, 0.3510, 0.3564]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9900, iteration 0\n",
            "q values:  tensor([[0.3739, 0.3861, 0.3714, 0.3595]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9900, iteration 1\n",
            "q values:  tensor([[0.3900, 0.3866, 0.3727, 0.3627]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9900, iteration 2\n",
            "q values:  tensor([[0.3984, 0.3808, 0.4059, 0.3665]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9900, iteration 3\n",
            "q values:  tensor([[0.3812, 0.3971, 0.4127, 0.3776]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9901, iteration 0\n",
            "q values:  tensor([[0.3827, 0.3843, 0.4104, 0.3785]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9901, iteration 1\n",
            "q values:  tensor([[0.3853, 0.3867, 0.4244, 0.3970]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9902, iteration 0\n",
            "q values:  tensor([[0.3831, 0.3646, 0.4310, 0.3860]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9902, iteration 1\n",
            "q values:  tensor([[0.3785, 0.3987, 0.4364, 0.4035]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9902, iteration 2\n",
            "q values:  tensor([[0.3820, 0.4135, 0.4341, 0.3985]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9902, iteration 3\n",
            "q values:  tensor([[0.4064, 0.4041, 0.4324, 0.3877]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9902, iteration 4\n",
            "q values:  tensor([[0.4012, 0.4042, 0.4158, 0.3894]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9902, iteration 5\n",
            "q values:  tensor([[0.3963, 0.4047, 0.3870, 0.3857]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9902, iteration 6\n",
            "q values:  tensor([[0.4067, 0.3993, 0.3876, 0.3832]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9902, iteration 7\n",
            "q values:  tensor([[0.3901, 0.4087, 0.3792, 0.3814]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9902, iteration 8\n",
            "q values:  tensor([[0.3851, 0.4031, 0.3847, 0.3827]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9902, iteration 9\n",
            "q values:  tensor([[0.3865, 0.3864, 0.3772, 0.3802]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9902, iteration 10\n",
            "q values:  tensor([[0.3968, 0.3816, 0.3640, 0.3672]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9902, iteration 11\n",
            "q values:  tensor([[0.3820, 0.3848, 0.3596, 0.3766]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9902, iteration 12\n",
            "q values:  tensor([[0.3847, 0.3914, 0.3610, 0.3634]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9903, iteration 0\n",
            "q values:  tensor([[0.3713, 0.3818, 0.3658, 0.3621]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9903, iteration 1\n",
            "q values:  tensor([[0.3919, 0.3922, 0.3644, 0.3617]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9903, iteration 2\n",
            "q values:  tensor([[0.3771, 0.3938, 0.3650, 0.3621]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9903, iteration 3\n",
            "q values:  tensor([[0.3747, 0.4039, 0.3707, 0.3616]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9904, iteration 0\n",
            "q values:  tensor([[0.3716, 0.3987, 0.3939, 0.3582]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9904, iteration 1\n",
            "q values:  tensor([[0.3752, 0.4082, 0.3986, 0.3575]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9905, iteration 0\n",
            "q values:  tensor([[0.3730, 0.3967, 0.3886, 0.3617]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9905, iteration 1\n",
            "q values:  tensor([[0.3890, 0.3982, 0.3912, 0.3555]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9905, iteration 2\n",
            "q values:  tensor([[0.3884, 0.3935, 0.3879, 0.3608]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9905, iteration 3\n",
            "q values:  tensor([[0.3846, 0.3839, 0.3907, 0.3811]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9906, iteration 0\n",
            "q values:  tensor([[0.3703, 0.3854, 0.4069, 0.3732]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9906, iteration 1\n",
            "q values:  tensor([[0.3701, 0.3898, 0.4015, 0.3755]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9906, iteration 2\n",
            "q values:  tensor([[0.3700, 0.3798, 0.3968, 0.3595]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9906, iteration 3\n",
            "q values:  tensor([[0.3677, 0.3717, 0.3951, 0.3658]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9907, iteration 0\n",
            "q values:  tensor([[0.3706, 0.3719, 0.4053, 0.3571]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9907, iteration 1\n",
            "q values:  tensor([[0.3596, 0.3698, 0.3999, 0.3638]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9907, iteration 2\n",
            "q values:  tensor([[0.3574, 0.3821, 0.3898, 0.3711]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9907, iteration 3\n",
            "q values:  tensor([[0.3775, 0.3781, 0.3874, 0.3612]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9907, iteration 4\n",
            "q values:  tensor([[0.3755, 0.3906, 0.3893, 0.3682]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9907, iteration 5\n",
            "q values:  tensor([[0.3680, 0.4014, 0.3926, 0.3650]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 6\n",
            "q values:  tensor([[0.3658, 0.3845, 0.4032, 0.3700]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 7\n",
            "q values:  tensor([[0.3638, 0.3882, 0.3925, 0.3673]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 8\n",
            "q values:  tensor([[0.3708, 0.3895, 0.3823, 0.3571]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 9\n",
            "q values:  tensor([[0.3706, 0.4007, 0.3734, 0.3556]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 10\n",
            "q values:  tensor([[0.3676, 0.4304, 0.3854, 0.3581]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 11\n",
            "q values:  tensor([[0.3812, 0.4289, 0.3824, 0.3523]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 12\n",
            "q values:  tensor([[0.3836, 0.4273, 0.3951, 0.3532]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 13\n",
            "q values:  tensor([[0.3859, 0.4346, 0.4245, 0.3616]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 14\n",
            "q values:  tensor([[0.3883, 0.4343, 0.4362, 0.3604]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 15\n",
            "q values:  tensor([[0.3907, 0.4329, 0.4330, 0.3781]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 16\n",
            "q values:  tensor([[0.3894, 0.4236, 0.4254, 0.3913]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 17\n",
            "q values:  tensor([[0.3858, 0.4162, 0.4264, 0.4036]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 18\n",
            "q values:  tensor([[0.4045, 0.4095, 0.4391, 0.4136]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9907, iteration 19\n",
            "q values:  tensor([[0.4052, 0.4096, 0.4351, 0.4162]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 20\n",
            "q values:  tensor([[0.3999, 0.4122, 0.4451, 0.4022]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 21\n",
            "q values:  tensor([[0.4038, 0.4121, 0.4355, 0.3967]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 22\n",
            "q values:  tensor([[0.4045, 0.3997, 0.4214, 0.3982]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 23\n",
            "q values:  tensor([[0.3933, 0.4095, 0.4147, 0.3932]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 24\n",
            "q values:  tensor([[0.3898, 0.4034, 0.4143, 0.3963]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 25\n",
            "q values:  tensor([[0.3859, 0.3862, 0.4027, 0.3853]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 26\n",
            "q values:  tensor([[0.3771, 0.3829, 0.3923, 0.3805]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 27\n",
            "q values:  tensor([[0.3600, 0.3945, 0.3936, 0.3830]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 28\n",
            "q values:  tensor([[0.3786, 0.3968, 0.3779, 0.3667]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 29\n",
            "q values:  tensor([[0.3808, 0.3920, 0.3723, 0.3717]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9907, iteration 30\n",
            "q values:  tensor([[0.3868, 0.4112, 0.3978, 0.3743]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9908, iteration 0\n",
            "q values:  tensor([[0.3773, 0.4282, 0.3988, 0.3648]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9908, iteration 1\n",
            "q values:  tensor([[0.3792, 0.4255, 0.3888, 0.3527]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9908, iteration 2\n",
            "q values:  tensor([[0.3759, 0.4394, 0.3961, 0.3506]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9909, iteration 0\n",
            "q values:  tensor([[0.3870, 0.4309, 0.4062, 0.3449]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9909, iteration 1\n",
            "q values:  tensor([[0.4034, 0.4169, 0.3955, 0.3696]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9909, iteration 2\n",
            "q values:  tensor([[0.3813, 0.4163, 0.3864, 0.3729]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9910, iteration 0\n",
            "q values:  tensor([[0.3867, 0.4155, 0.3784, 0.3855]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9910, iteration 1\n",
            "q values:  tensor([[0.3770, 0.4174, 0.3754, 0.3766]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9910, iteration 2\n",
            "q values:  tensor([[0.3775, 0.4052, 0.3769, 0.3798]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9911, iteration 0\n",
            "q values:  tensor([[0.3709, 0.4200, 0.3792, 0.4153]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9911, iteration 1\n",
            "q values:  tensor([[0.3684, 0.4200, 0.3870, 0.4249]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9911, iteration 2\n",
            "q values:  tensor([[0.3862, 0.4199, 0.3906, 0.4049]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9911, iteration 3\n",
            "q values:  tensor([[0.3900, 0.4075, 0.3953, 0.3933]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9912, iteration 0\n",
            "q values:  tensor([[0.3925, 0.4253, 0.3980, 0.3997]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9912, iteration 1\n",
            "q values:  tensor([[0.4035, 0.4185, 0.4007, 0.4019]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9912, iteration 2\n",
            "q values:  tensor([[0.3934, 0.4187, 0.3908, 0.4186]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9912, iteration 3\n",
            "q values:  tensor([[0.4036, 0.4126, 0.3924, 0.4188]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9913, iteration 0\n",
            "q values:  tensor([[0.4326, 0.4098, 0.4076, 0.4340]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9913, iteration 1\n",
            "q values:  tensor([[0.4281, 0.3921, 0.3973, 0.4399]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9913, iteration 2\n",
            "q values:  tensor([[0.4278, 0.3937, 0.3902, 0.4393]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9913, iteration 3\n",
            "q values:  tensor([[0.4154, 0.3960, 0.3864, 0.4248]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9914, iteration 0\n",
            "q values:  tensor([[0.4239, 0.3923, 0.4036, 0.4242]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9914, iteration 1\n",
            "q values:  tensor([[0.4449, 0.3967, 0.4129, 0.4170]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9915, iteration 0\n",
            "q values:  tensor([[0.4500, 0.3825, 0.4214, 0.4173]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9915, iteration 1\n",
            "q values:  tensor([[0.4298, 0.3812, 0.4212, 0.4047]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9915, iteration 2\n",
            "q values:  tensor([[0.4291, 0.3869, 0.4292, 0.4008]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9915, iteration 3\n",
            "q values:  tensor([[0.4112, 0.3845, 0.4219, 0.3957]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9915, iteration 4\n",
            "q values:  tensor([[0.4064, 0.4035, 0.4358, 0.3943]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9915, iteration 5\n",
            "q values:  tensor([[0.4024, 0.4326, 0.4485, 0.3925]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9915, iteration 6\n",
            "q values:  tensor([[0.3986, 0.4191, 0.4377, 0.3872]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9915, iteration 7\n",
            "q values:  tensor([[0.4006, 0.4132, 0.4499, 0.4052]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9915, iteration 8\n",
            "q values:  tensor([[0.4166, 0.4077, 0.4614, 0.3881]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9915, iteration 9\n",
            "q values:  tensor([[0.4248, 0.4083, 0.4695, 0.3920]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9915, iteration 10\n",
            "q values:  tensor([[0.4129, 0.4235, 0.4530, 0.3829]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9915, iteration 11\n",
            "q values:  tensor([[0.4137, 0.4171, 0.4373, 0.3804]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9915, iteration 12\n",
            "q values:  tensor([[0.4088, 0.4175, 0.4498, 0.3935]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9915, iteration 13\n",
            "q values:  tensor([[0.4183, 0.4057, 0.4355, 0.4100]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9915, iteration 14\n",
            "q values:  tensor([[0.4071, 0.4061, 0.4485, 0.4046]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9915, iteration 15\n",
            "q values:  tensor([[0.3919, 0.4065, 0.4340, 0.4007]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9915, iteration 16\n",
            "q values:  tensor([[0.3942, 0.4080, 0.4395, 0.3895]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9915, iteration 17\n",
            "q values:  tensor([[0.4175, 0.3970, 0.4314, 0.3928]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9915, iteration 18\n",
            "q values:  tensor([[0.4309, 0.3891, 0.4242, 0.3940]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9916, iteration 0\n",
            "q values:  tensor([[0.4259, 0.3852, 0.4373, 0.3911]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9916, iteration 1\n",
            "q values:  tensor([[0.4033, 0.3971, 0.4292, 0.3798]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9916, iteration 2\n",
            "q values:  tensor([[0.3939, 0.4076, 0.4277, 0.3988]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9916, iteration 3\n",
            "q values:  tensor([[0.3924, 0.4021, 0.4214, 0.3958]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9917, iteration 0\n",
            "q values:  tensor([[0.3884, 0.4011, 0.4213, 0.3806]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9917, iteration 1\n",
            "q values:  tensor([[0.3756, 0.3969, 0.4153, 0.3936]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9917, iteration 2\n",
            "q values:  tensor([[0.3638, 0.3923, 0.4043, 0.4138]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9917, iteration 3\n",
            "q values:  tensor([[0.3674, 0.4037, 0.3972, 0.4144]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9918, iteration 0\n",
            "q values:  tensor([[0.3674, 0.4051, 0.3942, 0.4300]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9918, iteration 1\n",
            "q values:  tensor([[0.3708, 0.4073, 0.4054, 0.4164]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9919, iteration 0\n",
            "q values:  tensor([[0.3898, 0.3962, 0.4154, 0.4041]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9919, iteration 1\n",
            "q values:  tensor([[0.3765, 0.3969, 0.4089, 0.3937]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9919, iteration 2\n",
            "q values:  tensor([[0.3756, 0.3990, 0.4101, 0.3851]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9919, iteration 3\n",
            "q values:  tensor([[0.3712, 0.3951, 0.4058, 0.4043]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9920, iteration 0\n",
            "q values:  tensor([[0.3776, 0.3902, 0.3955, 0.4004]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9920, iteration 1\n",
            "q values:  tensor([[0.3857, 0.3892, 0.3976, 0.3956]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9920, iteration 2\n",
            "q values:  tensor([[0.3786, 0.3870, 0.4004, 0.3974]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9920, iteration 3\n",
            "q values:  tensor([[0.3980, 0.4060, 0.4015, 0.4138]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9921, iteration 0\n",
            "q values:  tensor([[0.4027, 0.4073, 0.4035, 0.4290]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9921, iteration 1\n",
            "q values:  tensor([[0.3942, 0.4033, 0.4027, 0.4227]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9921, iteration 2\n",
            "q values:  tensor([[0.3955, 0.4006, 0.4006, 0.4234]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9922, iteration 0\n",
            "q values:  tensor([[0.4000, 0.4054, 0.3968, 0.4176]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9922, iteration 1\n",
            "q values:  tensor([[0.3970, 0.4177, 0.3823, 0.4466]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9922, iteration 2\n",
            "q values:  tensor([[0.4071, 0.4054, 0.3786, 0.4448]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9922, iteration 3\n",
            "q values:  tensor([[0.4085, 0.4086, 0.3801, 0.4175]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9922, iteration 4\n",
            "q values:  tensor([[0.3998, 0.4114, 0.3980, 0.4126]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9922, iteration 5\n",
            "q values:  tensor([[0.4172, 0.4138, 0.3892, 0.4269]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 6\n",
            "q values:  tensor([[0.4258, 0.4308, 0.3887, 0.4152]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 7\n",
            "q values:  tensor([[0.4146, 0.4393, 0.3927, 0.4192]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 8\n",
            "q values:  tensor([[0.3940, 0.4312, 0.3901, 0.4338]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 9\n",
            "q values:  tensor([[0.4109, 0.4331, 0.3709, 0.4273]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 10\n",
            "q values:  tensor([[0.4012, 0.4126, 0.3909, 0.4214]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 11\n",
            "q values:  tensor([[0.4037, 0.4150, 0.3880, 0.4163]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 12\n",
            "q values:  tensor([[0.4004, 0.4095, 0.4047, 0.4051]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 13\n",
            "q values:  tensor([[0.3824, 0.4061, 0.4064, 0.4067]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 14\n",
            "q values:  tensor([[0.3986, 0.4012, 0.4025, 0.4242]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 15\n",
            "q values:  tensor([[0.3980, 0.4178, 0.4040, 0.4388]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9922, iteration 16\n",
            "q values:  tensor([[0.3949, 0.4152, 0.4065, 0.4457]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9922, iteration 17\n",
            "q values:  tensor([[0.3946, 0.4163, 0.3960, 0.4318]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9922, iteration 18\n",
            "q values:  tensor([[0.3870, 0.4178, 0.3984, 0.4139]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9922, iteration 19\n",
            "q values:  tensor([[0.3879, 0.4185, 0.4019, 0.4230]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9922, iteration 20\n",
            "q values:  tensor([[0.3860, 0.4127, 0.3980, 0.4012]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9922, iteration 21\n",
            "q values:  tensor([[0.3888, 0.4082, 0.3899, 0.4056]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9922, iteration 22\n",
            "q values:  tensor([[0.4023, 0.4248, 0.3757, 0.3916]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9922, iteration 23\n",
            "q values:  tensor([[0.4190, 0.4250, 0.3756, 0.3898]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 0\n",
            "q values:  tensor([[0.4089, 0.3936, 0.3939, 0.3919]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9923, iteration 1\n",
            "q values:  tensor([[0.4112, 0.3973, 0.4112, 0.3848]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9923, iteration 2\n",
            "q values:  tensor([[0.4070, 0.3991, 0.4117, 0.3907]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9923, iteration 3\n",
            "q values:  tensor([[0.4039, 0.4041, 0.4067, 0.3770]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9923, iteration 4\n",
            "q values:  tensor([[0.3943, 0.3990, 0.4129, 0.3833]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9923, iteration 5\n",
            "q values:  tensor([[0.3913, 0.3952, 0.4161, 0.3811]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9923, iteration 6\n",
            "q values:  tensor([[0.3793, 0.3993, 0.4340, 0.3837]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9923, iteration 7\n",
            "q values:  tensor([[0.3851, 0.4130, 0.4334, 0.3868]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  0.0\n",
            "episode 9923, iteration 8\n",
            "q values:  tensor([[0.3817, 0.4018, 0.4472, 0.4078]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 9\n",
            "q values:  tensor([[0.3728, 0.4205, 0.4440, 0.4037]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 10\n",
            "q values:  tensor([[0.3673, 0.4240, 0.4299, 0.4050]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 11\n",
            "q values:  tensor([[0.3731, 0.4180, 0.4235, 0.3966]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 12\n",
            "q values:  tensor([[0.3775, 0.4198, 0.4323, 0.3996]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 13\n",
            "q values:  tensor([[0.3773, 0.4204, 0.4541, 0.3993]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 14\n",
            "q values:  tensor([[0.4047, 0.4216, 0.4430, 0.4013]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 15\n",
            "q values:  tensor([[0.4075, 0.4247, 0.4359, 0.4250]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 16\n",
            "q values:  tensor([[0.4225, 0.4253, 0.4286, 0.4251]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 17\n",
            "q values:  tensor([[0.4066, 0.4190, 0.4631, 0.4195]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 18\n",
            "q values:  tensor([[0.4041, 0.4194, 0.4476, 0.4090]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 19\n",
            "q values:  tensor([[0.4054, 0.4138, 0.4529, 0.4485]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 20\n",
            "q values:  tensor([[0.4150, 0.4024, 0.4459, 0.4653]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 21\n",
            "q values:  tensor([[0.4181, 0.3934, 0.4385, 0.4572]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  1.0\n",
            "episode 9923, iteration 22\n",
            "q values:  tensor([[0.4133, 0.3911, 0.4314, 0.4490]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([1.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 23\n",
            "q values:  tensor([[0.4145, 0.4359, 0.4139, 0.4361]], grad_fn=<AddmmBackward>)\n",
            "Action:  1\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 24\n",
            "q values:  tensor([[0.4156, 0.4351, 0.4105, 0.4302]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 25\n",
            "q values:  tensor([[0.4058, 0.4280, 0.4009, 0.4248]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 26\n",
            "q values:  tensor([[0.3968, 0.4363, 0.4241, 0.4494]], grad_fn=<AddmmBackward>)\n",
            "Action:  2\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 27\n",
            "q values:  tensor([[0.3940, 0.4372, 0.4246, 0.4427]], grad_fn=<AddmmBackward>)\n",
            "Action:  3\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 28\n",
            "q values:  tensor([[0.3815, 0.4362, 0.4250, 0.4362]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 29\n",
            "q values:  tensor([[0.3808, 0.4290, 0.4253, 0.4478]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 30\n",
            "q values:  tensor([[0.3866, 0.4227, 0.4200, 0.4355]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 31\n",
            "q values:  tensor([[0.3960, 0.4402, 0.4153, 0.4350]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n",
            "episode 9923, iteration 32\n",
            "q values:  tensor([[0.3884, 0.4335, 0.4115, 0.4238]], grad_fn=<AddmmBackward>)\n",
            "Action:  0\n",
            "reward:  tensor([0.], dtype=torch.float64)\n",
            "current total score:  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxu7rHP2P_i8"
      },
      "source": [
        "filtered_durations = []\n",
        "for i in range(len(episode_durations)):\n",
        "  if i % 100 == 0:\n",
        "    filtered_durations.append(episode_durations[i])\n",
        "\n",
        "plt.title('Iterations per Episode')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Iterations (per 100)')\n",
        "plt.plot(filtered_durations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xG5PlK44ohz"
      },
      "source": [
        "filtered_runtime = []\n",
        "for i in range(len(all_episode_training_time)):\n",
        "  if i % 100 == 0:\n",
        "    filtered_runtime.append(all_episode_training_time[i])\n",
        "\n",
        "plt.title('Runtime(s) per Episode')\n",
        "plt.xlabel('Episode (per 100)')\n",
        "plt.ylabel('Runtime(s)')\n",
        "plt.plot(filtered_runtime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIEDNhOU5rCE"
      },
      "source": [
        "filtered_scores = []\n",
        "for i in range(len(all_episode_scores)):\n",
        "  if i % 100 == 0:\n",
        "    filtered_scores.append(all_episode_scores[i])\n",
        "plt.title('Score per Episode')\n",
        "plt.xlabel('Episode (per 100)')\n",
        "plt.ylabel('Score')\n",
        "plt.plot(filtered_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbtx-nG34rwF"
      },
      "source": [
        "# calculate average max q value per episode\n",
        "avg_max_qVals = []\n",
        "for vals in all_episode_max_q_vals:\n",
        "    avg_max_qVal = sum(vals)/len(vals)\n",
        "    avg_max_qVals.append(avg_max_qVal)\n",
        "\n",
        "filtered_max_q_value = []\n",
        "for i in range(len(avg_max_qVals)):\n",
        "  if i % 100 == 0:\n",
        "    filtered_max_q_value.append(avg_max_qVals[i])\n",
        "plt.title('Average Max Q value per Episode')\n",
        "plt.xlabel('Episode (per 100)')\n",
        "plt.ylabel('Average Max Q value')\n",
        "plt.plot(filtered_max_q_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQmnfPKi52Yj"
      },
      "source": [
        "## Record"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stYYM0LZ6kVe"
      },
      "source": [
        "video_save_interval = 1\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "      mp4 = mp4list[0]\n",
        "      video = io.open(mp4, 'r+b').read()\n",
        "      encoded = base64.b64encode(video)\n",
        "      ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                  loop controls style=\"height: 400px;\">\n",
        "                  <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "              </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "      print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(envi):\n",
        "    envi = Monitor(envi, './video', video_callable=lambda episode_id: episode_id % video_save_interval == 0, force=True)\n",
        "    return envi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdiAUJAH55tE"
      },
      "source": [
        "# Initialize the environment and state\n",
        "record_env = gym.make('Breakout-v0').unwrapped\n",
        "record_env = WrapAtariEnv(record_env)\n",
        "record_env = wrap_env(record_env)\n",
        "record_env.reset()\n",
        "train(1, record_env)\n",
        "print(\"Score: \", all_episode_scores[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q8QTT1Y6N_i"
      },
      "source": [
        "record_env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49dELtih7ZB2"
      },
      "source": [
        "avg_max_qVal = sum(all_episode_max_q_vals[-1])/len(all_episode_max_q_vals[-1])\n",
        "avg_max_qVal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBn-Ucl1fjt"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Et-r2tD1n0m"
      },
      "source": [
        "video_save_interval = 1\n",
        "# Initialize the environment and state\n",
        "total_score = 0\n",
        "test_env = gym.make('Breakout-v0').unwrapped\n",
        "test_env = WrapAtariEnv(test_env, test = True)\n",
        "test_env = wrap_env(test_env)\n",
        "test_env.reset()\n",
        "\n",
        "# Stack 4 most recent frames\n",
        "past_ob3, _, _, _= test_env.step(0) # 0 -> no op\n",
        "past_ob2, _, _, _= test_env.step(0)\n",
        "past_ob1, _, _, _ = test_env.step(0)\n",
        "current_ob, _, _, _ = test_env.step(0)\n",
        "state = stack_past_four_frames(current_ob, past_ob1, past_ob2, past_ob3)\n",
        "\n",
        "real_done = False\n",
        "\n",
        "start = time.time()\n",
        "for t in count():\n",
        "    print(\"iteration {}\".format(t))\n",
        "\n",
        "    test_env.render()     \n",
        "    # Compute Q values from policy net\n",
        "    q_values = get_q_values(state)\n",
        "    print(\"q values: \", q_values)\n",
        "\n",
        "    # Choose action based on max q value\n",
        "    action = select_action(q_values)\n",
        "    print(\"Action: \", action.item())\n",
        "\n",
        "    # Execute action\n",
        "    new_ob, reward, done, info = test_env.step(action.item())\n",
        "    total_score += reward\n",
        "    print(\"current total score: \", total_score)\n",
        "\n",
        "    # Observe new state\n",
        "    past_ob3 = past_ob2\n",
        "    past_ob2 = past_ob1\n",
        "    past_ob1 = current_ob\n",
        "    current_ob = new_ob\n",
        "    \n",
        "    # check if game ended\n",
        "    lives_left = test_env.env.unwrapped.ale.lives()  # how many lives left\n",
        "    if lives_left <= 0:\n",
        "       real_done = True\n",
        "    \n",
        "    if not real_done:\n",
        "        next_state = stack_past_four_frames(current_ob, past_ob1, past_ob2, past_ob3)\n",
        "    else:\n",
        "        next_state = None\n",
        "\n",
        "    # Move to the next state\n",
        "    state = next_state\n",
        "\n",
        "    if real_done or t > 1000:\n",
        "        end = time.time()\n",
        "        print(\"Total testing runtime in seconds: \", end - start)\n",
        "        break\n",
        "print('Testing Complete, total score: ', total_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n3ddD858E8d"
      },
      "source": [
        "test_env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}